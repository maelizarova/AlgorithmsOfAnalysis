{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тематизация жалоб без эмбеддингов: LDA и LSI+KMeans\n",
        "\n",
        "Этот ноутбук содержит полностью рабочие пайплайны без использования эмбеддингов HuggingFace:\n",
        "- LDA (CountVectorizer → LatentDirichletAllocation)\n",
        "- LSI (TF‑IDF → TruncatedSVD) + KMeans\n",
        "\n",
        "Включено:\n",
        "- Очистка текста (минимальная, с заменой PII)\n",
        "- Обучение, предсказание, извлечение ключевых слов для тем/кластеров\n",
        "- Метрики: LDA перплексия, silhouette для KMeans\n",
        "- Сохранение/загрузка моделей\n",
        "- Демонстрация на примерах\n",
        "\n",
        "Подойдёт как стартовый шаблон для кластеризации обращений клиентов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# При необходимости раскомментируйте для установки\n",
        "# %pip install scikit-learn pandas numpy joblib razdel\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Токены, которые не должны попадать в названия тем/топ-термины\n",
        "BLOCKED_TERMS = {\n",
        "    \"amount\", \"card\", \"date\", \"email\",\n",
        "    \"pii_amount\", \"pii_card\", \"pii_date\", \"pii_email\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "\n",
        "def load_texts_from_csv(csv_path: str, text_col: str = \"text\") -> List[str]:\n",
        "    \"\"\"Загрузить столбец с текстами из CSV.\n",
        "    Ожидается, что в файле есть колонка с названием text_col.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert text_col in df.columns, f\"В CSV нет колонки '{text_col}'\"\n",
        "    texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
        "    return texts\n",
        "\n",
        "# Пример использования:\n",
        "# texts = load_texts_from_csv(\"complaints.csv\", text_col=\"complaint\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Очистка текста\n",
        "\n",
        "PII_EMAIL = re.compile(r\"\\S+@\\S+\")\n",
        "PII_CARD = re.compile(r\"\\b\\d{12,16}\\b\")\n",
        "PII_DATE = re.compile(r\"\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\")\n",
        "PII_AMOUNT = re.compile(r\"\\d+[ ,\\.]?\\d*\\s?(₽|р\\.?|rub|\\$|usd|eur)?\", re.IGNORECASE)\n",
        "\n",
        "RE_MULTISPACE = re.compile(r\"\\s{2,}\")\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Минимальная нормализация и замена PII.\"\"\"\n",
        "    t = str(text).lower()\n",
        "    t = PII_EMAIL.sub(\"<email>\", t)\n",
        "    t = PII_CARD.sub(\"<card>\", t)\n",
        "    t = PII_DATE.sub(\"<date>\", t)\n",
        "    t = PII_AMOUNT.sub(\"<amount>\", t)\n",
        "    t = RE_MULTISPACE.sub(\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "\n",
        "def clean_corpus(texts: List[str]) -> List[str]:\n",
        "    return [clean_text(t) for t in texts]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пайплайн LDA\n",
        "\n",
        "class LdaTopicModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_topics: int = 10,\n",
        "        max_df: float = 0.95,\n",
        "        min_df: int = 5,\n",
        "        ngram_range: Tuple[int, int] = (1, 2),\n",
        "        random_state: int = 42,\n",
        "    ) -> None:\n",
        "        self.n_topics = n_topics\n",
        "        self.vectorizer = CountVectorizer(\n",
        "            stop_words=\"russian\",\n",
        "            max_df=max_df,\n",
        "            min_df=min_df,\n",
        "            ngram_range=ngram_range,\n",
        "        )\n",
        "        self.lda = LatentDirichletAllocation(\n",
        "            n_components=n_topics,\n",
        "            learning_method=\"batch\",\n",
        "            max_iter=20,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, texts: List[str]) -> \"LdaTopicModel\":\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X = self.vectorizer.fit_transform(cleaned)\n",
        "        self.doc_topic = self.lda.fit_transform(X)\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X = self.vectorizer.transform(cleaned)\n",
        "        return self.lda.transform(X)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        dist = self.transform(texts)\n",
        "        return dist.argmax(axis=1), dist.max(axis=1)\n",
        "\n",
        "    def get_topic_top_words(self, top_n: int = 10) -> Dict[int, List[str]]:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "        topics: Dict[int, List[str]] = {}\n",
        "        for i, comp in enumerate(self.lda.components_):\n",
        "            top_idx = comp.argsort()[::-1]\n",
        "            words: List[str] = []\n",
        "            for j in top_idx:\n",
        "                w = feature_names[j]\n",
        "                if w in BLOCKED_TERMS:\n",
        "                    continue\n",
        "                words.append(w)\n",
        "                if len(words) >= top_n:\n",
        "                    break\n",
        "            topics[i] = words\n",
        "        return topics\n",
        "\n",
        "    def to_dataframe(self, texts: List[str]) -> pd.DataFrame:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        preds, conf = self.predict(texts)\n",
        "        return pd.DataFrame({\"text\": texts, \"topic_id\": preds, \"topic_confidence\": conf})\n",
        "\n",
        "    def perplexity(self, texts: Optional[List[str]] = None) -> float:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        if texts is None:\n",
        "            texts = self._last_fit_corpus if hasattr(self, \"_last_fit_corpus\") else None\n",
        "        cleaned = clean_corpus(texts) if texts is not None else None\n",
        "        X = self.vectorizer.transform(cleaned) if cleaned is not None else None\n",
        "        if X is None:\n",
        "            raise ValueError(\"Передайте texts для оценки перплексии\")\n",
        "        return float(self.lda.perplexity(X))\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        joblib.dump(self.vectorizer, os.path.join(path, \"vectorizer.joblib\"))\n",
        "        joblib.dump(self.lda, os.path.join(path, \"lda.joblib\"))\n",
        "        meta = {\"n_topics\": self.n_topics}\n",
        "        with open(os.path.join(path, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"LdaTopicModel\":\n",
        "        with open(os.path.join(path, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "        obj = cls(n_topics=meta[\"n_topics\"])\n",
        "        obj.vectorizer = joblib.load(os.path.join(path, \"vectorizer.joblib\"))\n",
        "        obj.lda = joblib.load(os.path.join(path, \"lda.joblib\"))\n",
        "        obj.is_fitted = True\n",
        "        return obj\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пайплайн LSI (TF-IDF + SVD) + KMeans\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class LsiKMeansTopicModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_components: int = 100,\n",
        "        n_clusters: int = 10,\n",
        "        max_df: float = 0.95,\n",
        "        min_df: int = 5,\n",
        "        ngram_range: Tuple[int, int] = (1, 2),\n",
        "        random_state: int = 42,\n",
        "    ) -> None:\n",
        "        self.n_components = n_components\n",
        "        self.n_clusters = n_clusters\n",
        "        self.tfidf = TfidfVectorizer(\n",
        "            stop_words=\"russian\",\n",
        "            max_df=max_df,\n",
        "            min_df=min_df,\n",
        "            ngram_range=ngram_range,\n",
        "        )\n",
        "        self.svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
        "        self.lsi = make_pipeline(self.svd, Normalizer(copy=False))\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, texts: List[str]) -> \"LsiKMeansTopicModel\":\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X_tfidf = self.tfidf.fit_transform(cleaned)\n",
        "        X_lsi = self.lsi.fit_transform(X_tfidf)\n",
        "        self.labels_ = self.kmeans.fit_predict(X_lsi)\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X_tfidf = self.tfidf.transform(cleaned)\n",
        "        return self.lsi.transform(X_tfidf)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> np.ndarray:\n",
        "        X_lsi = self.transform(texts)\n",
        "        return self.kmeans.predict(X_lsi)\n",
        "\n",
        "    def to_dataframe(self, texts: List[str]) -> pd.DataFrame:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        labels = self.predict(texts)\n",
        "        return pd.DataFrame({\"text\": texts, \"topic_id\": labels})\n",
        "\n",
        "    def silhouette(self, texts: List[str]) -> float:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        X_lsi = self.transform(texts)\n",
        "        return float(silhouette_score(X_lsi, self.kmeans.predict(X_lsi)))\n",
        "\n",
        "    def get_cluster_terms(self, texts: List[str], top_n: int = 10) -> Dict[int, List[str]]:\n",
        "        \"\"\"c-TF-IDF по агрегированным документам кластеров.\n",
        "        Передайте исходные тексты корпуса (те же, на которых обучали).\n",
        "        \"\"\"\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        # Аггрегируем по кластерам\n",
        "        cluster_docs: Dict[int, List[str]] = defaultdict(list)\n",
        "        for t, lab in zip(cleaned, self.labels_):\n",
        "            cluster_docs[lab].append(t)\n",
        "        agg_texts = [\" \".join(cluster_docs[i]) if i in cluster_docs else \"\" for i in range(self.n_clusters)]\n",
        "        # Считаем c-TF-IDF\n",
        "        ctfidf = TfidfVectorizer(stop_words=\"russian\", ngram_range=(1, 2), min_df=2)\n",
        "        C = ctfidf.fit_transform(agg_texts)\n",
        "        feat = ctfidf.get_feature_names_out()\n",
        "        terms: Dict[int, List[str]] = {}\n",
        "        for i in range(self.n_clusters):\n",
        "            row = C[i]\n",
        "            scores = row.toarray().ravel()\n",
        "            order = scores.argsort()[::-1]\n",
        "            words: List[str] = []\n",
        "            for j in order:\n",
        "                w = str(feat[j])\n",
        "                if w in BLOCKED_TERMS:\n",
        "                    continue\n",
        "                words.append(w)\n",
        "                if len(words) >= top_n:\n",
        "                    break\n",
        "            terms[i] = words\n",
        "        return terms\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        joblib.dump(self.tfidf, os.path.join(path, \"tfidf.joblib\"))\n",
        "        joblib.dump(self.svd, os.path.join(path, \"svd.joblib\"))\n",
        "        joblib.dump(self.lsi, os.path.join(path, \"lsi_pipeline.joblib\"))\n",
        "        joblib.dump(self.kmeans, os.path.join(path, \"kmeans.joblib\"))\n",
        "        meta = {\"n_components\": self.n_components, \"n_clusters\": self.n_clusters}\n",
        "        with open(os.path.join(path, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"LsiKMeansTopicModel\":\n",
        "        with open(os.path.join(path, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "        obj = cls(n_components=meta[\"n_components\"], n_clusters=meta[\"n_clusters\"])\n",
        "        obj.tfidf = joblib.load(os.path.join(path, \"tfidf.joblib\"))\n",
        "        obj.svd = joblib.load(os.path.join(path, \"svd.joblib\"))\n",
        "        obj.lsi = joblib.load(os.path.join(path, \"lsi_pipeline.joblib\"))\n",
        "        obj.kmeans = joblib.load(os.path.join(path, \"kmeans.joblib\"))\n",
        "        obj.is_fitted = True\n",
        "        return obj\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подбор числа тем/кластеров\n",
        "\n",
        "def lda_try_topics(texts: List[str], topics_list: List[int]) -> pd.DataFrame:\n",
        "    \"\"\"Перебор числа тем для LDA и оценка перплексии (ниже — лучше).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    cv = CountVectorizer(stop_words=\"russian\", max_df=0.95, min_df=5, ngram_range=(1, 2))\n",
        "    X = cv.fit_transform(cleaned)\n",
        "    rows = []\n",
        "    for k in topics_list:\n",
        "        lda = LatentDirichletAllocation(n_components=k, learning_method=\"batch\", max_iter=15, random_state=42, n_jobs=-1)\n",
        "        lda.fit(X)\n",
        "        perp = float(lda.perplexity(X))\n",
        "        rows.append({\"n_topics\": k, \"perplexity\": perp})\n",
        "    return pd.DataFrame(rows).sort_values(\"n_topics\")\n",
        "\n",
        "\n",
        "def lsi_kmeans_try_k(texts: List[str], k_list: List[int], n_components: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"Перебор k для KMeans на LSI-признаках и оценка silhouette (выше — лучше).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    tfidf = TfidfVectorizer(stop_words=\"russian\", max_df=0.95, min_df=5, ngram_range=(1, 2))\n",
        "    X_tfidf = tfidf.fit_transform(cleaned)\n",
        "    lsi = make_pipeline(TruncatedSVD(n_components=n_components, random_state=42), Normalizer(copy=False))\n",
        "    X_lsi = lsi.fit_transform(X_tfidf)\n",
        "    rows = []\n",
        "    for k in k_list:\n",
        "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
        "        labels = km.fit_predict(X_lsi)\n",
        "        sil = float(silhouette_score(X_lsi, labels))\n",
        "        rows.append({\"k\": k, \"silhouette\": sil})\n",
        "    return pd.DataFrame(rows).sort_values(\"k\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демонстрация на примерах\n",
        "\n",
        "example_texts = [\n",
        "    \"Клиент не согласен со списанием 525,41 р. за обслуживание карты (годовая комиссия) и просит вернуть средства\",\n",
        "    \"Перевод через систему не поступил получателю. Клиент просит сделать розыск и вернуть средства на карту\",\n",
        "    \"Была заблокирована карта без предупреждения, деньги недоступны, прошу разблокировать и вернуть доступ\",\n",
        "    \"Нужен перерасчет возврата страховки по досрочно погашенному кредиту, официальный ответ не получен\",\n",
        "    \"В мобильном приложении нет данных по лимиту карты несмотря на уведомление\",\n",
        "]\n",
        "\n",
        "# 1) LDA\n",
        "lda_model = LdaTopicModel(n_topics=10)\n",
        "lda_model.fit(example_texts)\n",
        "lda_topics_df = lda_model.to_dataframe(example_texts)\n",
        "print(\"Топ-слова по темам (LDA):\")\n",
        "print(pd.DataFrame.from_dict(lda_model.get_topic_top_words(top_n=8), orient=\"index\"))\n",
        "\n",
        "display(lda_topics_df)\n",
        "\n",
        "# 2) LSI+KMeans\n",
        "lsi_model = LsiKMeansTopicModel(n_components=100, n_clusters=10)\n",
        "lsi_model.fit(example_texts)\n",
        "lsi_topics_df = lsi_model.to_dataframe(example_texts)\n",
        "print(\"\\nТоп-термины по кластерам (LSI+KMeans):\")\n",
        "print(pd.DataFrame.from_dict(lsi_model.get_cluster_terms(example_texts, top_n=8), orient=\"index\"))\n",
        "\n",
        "display(lsi_topics_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Применение к новым жалобам + сохранение/загрузка\n",
        "\n",
        "new_texts = [\n",
        "    \"Прошу вернуть годовую комиссию, не было уведомления о списании\",\n",
        "    \"Перевод на казахстанскую карту не дошёл, сделайте розыск\",\n",
        "]\n",
        "\n",
        "lda_pred_ids, lda_pred_conf = lda_model.predict(new_texts)\n",
        "print(\"LDA предсказания:\")\n",
        "print(pd.DataFrame({\"text\": new_texts, \"topic_id\": lda_pred_ids, \"confidence\": lda_pred_conf}))\n",
        "\n",
        "lsi_pred_ids = lsi_model.predict(new_texts)\n",
        "print(\"\\nLSI+KMeans предсказания:\")\n",
        "print(pd.DataFrame({\"text\": new_texts, \"topic_id\": lsi_pred_ids}))\n",
        "\n",
        "# Сохранение\n",
        "lda_model.save(\"models/lda_model\")\n",
        "lsi_model.save(\"models/lsi_kmeans_model\")\n",
        "\n",
        "# Загрузка\n",
        "lda_loaded = LdaTopicModel.load(\"models/lda_model\")\n",
        "lsi_loaded = LsiKMeansTopicModel.load(\"models/lsi_kmeans_model\")\n",
        "\n",
        "_ = lda_loaded.predict([\"Возврат страховки по кредиту не произведён\"])  # smoke-test\n",
        "_ = lsi_loaded.predict([\"Карта заблокирована без предупреждения\"])  # smoke-test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пост-обработка: мэппинг тем в бизнес‑рубрики\n",
        "\n",
        "- Создаем словарь соответствия `исходная_тема → бизнес‑тема`.\n",
        "- LDA: суммируем вероятности по темам, входящим в одну бизнес‑тему, и берём максимум.\n",
        "- Пересчитываем ключевые термины по финальным бизнес‑темам (c‑TF‑IDF).\n",
        "- Анализируем размер финальных тем (шт. и % от корпуса).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def apply_lda_mapping(lda_model: LdaTopicModel, texts: List[str], topic_to_business: Dict[int, str]) -> pd.DataFrame:\n",
        "    \"\"\"Возвращает финальную бизнес-метку и уверенность (сумма вероятностей) по документу.\"\"\"\n",
        "    dist = lda_model.transform(texts)\n",
        "    final_labels: List[str] = []\n",
        "    final_conf: List[float] = []\n",
        "    for row in dist:\n",
        "        agg: Dict[str, float] = {}\n",
        "        for topic_id, p in enumerate(row):\n",
        "            label = topic_to_business.get(topic_id, f\"topic_{topic_id}\")\n",
        "            agg[label] = agg.get(label, 0.0) + float(p)\n",
        "        best_label, best_prob = max(agg.items(), key=lambda x: x[1])\n",
        "        final_labels.append(best_label)\n",
        "        final_conf.append(best_prob)\n",
        "    return pd.DataFrame({\"text\": texts, \"final_label\": final_labels, \"final_conf\": final_conf})\n",
        "\n",
        "\n",
        "def apply_kmeans_mapping(lsi_model: LsiKMeansTopicModel, texts: List[str], cluster_to_business: Dict[int, str]) -> pd.DataFrame:\n",
        "    labels = lsi_model.predict(texts)\n",
        "    final_labels = [cluster_to_business.get(int(l), f\"cluster_{int(l)}\") for l in labels]\n",
        "    return pd.DataFrame({\"text\": texts, \"final_label\": final_labels})\n",
        "\n",
        "\n",
        "def compute_business_terms(texts: List[str], labels: List[str], top_n: int = 10) -> Dict[str, List[str]]:\n",
        "    \"\"\"Считает ключевые термины для финальных бизнес‑тем (c‑TF‑IDF).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    groups: Dict[str, List[str]] = {}\n",
        "    for t, lab in zip(cleaned, labels):\n",
        "        groups.setdefault(lab, []).append(t)\n",
        "    business_names = sorted(groups.keys())\n",
        "    agg_texts = [\" \".join(groups[name]) for name in business_names]\n",
        "    vec = TfidfVectorizer(stop_words=\"russian\", ngram_range=(1, 2), min_df=2)\n",
        "    M = vec.fit_transform(agg_texts)\n",
        "    feat = vec.get_feature_names_out()\n",
        "    terms: Dict[str, List[str]] = {}\n",
        "    for idx, name in enumerate(business_names):\n",
        "        row = M[idx]\n",
        "        scores = row.toarray().ravel()\n",
        "        order = scores.argsort()[::-1]\n",
        "        words: List[str] = []\n",
        "        for j in order:\n",
        "            w = str(feat[j])\n",
        "            if w in BLOCKED_TERMS:\n",
        "                continue\n",
        "            words.append(w)\n",
        "            if len(words) >= top_n:\n",
        "                break\n",
        "        terms[name] = words\n",
        "    return terms\n",
        "\n",
        "\n",
        "def business_size_report(labels: List[str]) -> pd.DataFrame:\n",
        "    cnt = Counter(labels)\n",
        "    total = sum(cnt.values())\n",
        "    rows = [{\"business_label\": k, \"count\": v, \"share\": v / total} for k, v in sorted(cnt.items(), key=lambda x: -x[1])]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Пример использования (заполните mapping под ваши рубрики)\n",
        "# Мэппинг для LDA: исходная_тема -> бизнес‑тема\n",
        "lda_mapping_example = {i: f\"Тема_{i}\" for i in range(lda_model.lda.n_components)}\n",
        "lda_final_df = apply_lda_mapping(lda_model, example_texts, lda_mapping_example)\n",
        "lda_terms_final = compute_business_terms(example_texts, lda_final_df[\"final_label\"].tolist(), top_n=8)\n",
        "print(\"Финальные бизнес‑темы (LDA):\")\n",
        "print(business_size_report(lda_final_df[\"final_label\"].tolist()))\n",
        "print(pd.DataFrame.from_dict(lda_terms_final, orient=\"index\"))\n",
        "\n",
        "# Мэппинг для LSI+KMeans: кластер -> бизнес‑тема\n",
        "kmeans_mapping_example = {i: f\"Тема_{i}\" for i in range(lsi_model.n_clusters)}\n",
        "kmeans_final_df = apply_kmeans_mapping(lsi_model, example_texts, kmeans_mapping_example)\n",
        "kmeans_terms_final = compute_business_terms(example_texts, kmeans_final_df[\"final_label\"].tolist(), top_n=8)\n",
        "print(\"\\nФинальные бизнес‑темы (LSI+KMeans):\")\n",
        "print(business_size_report(kmeans_final_df[\"final_label\"].tolist()))\n",
        "print(pd.DataFrame.from_dict(kmeans_terms_final, orient=\"index\"))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
