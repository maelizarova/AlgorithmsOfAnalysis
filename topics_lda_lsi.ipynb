{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тематизация жалоб без эмбеддингов: LDA и LSI+KMeans\n",
        "\n",
        "Этот ноутбук содержит полностью рабочие пайплайны без использования эмбеддингов HuggingFace:\n",
        "- LDA (CountVectorizer → LatentDirichletAllocation)\n",
        "- LSI (TF‑IDF → TruncatedSVD) + KMeans\n",
        "\n",
        "Включено:\n",
        "- Очистка текста (минимальная, с заменой PII)\n",
        "- Обучение, предсказание, извлечение ключевых слов для тем/кластеров\n",
        "- Метрики: LDA перплексия, silhouette для KMeans\n",
        "- Сохранение/загрузка моделей\n",
        "- Демонстрация на примерах\n",
        "\n",
        "Подойдёт как стартовый шаблон для кластеризации обращений клиентов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# При необходимости раскомментируйте для установки\n",
        "# %pip install scikit-learn pandas numpy joblib razdel nltk\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Токены, которые не должны попадать в названия тем/топ-термины\n",
        "BLOCKED_TERMS = {\n",
        "    \"amount\", \"card\", \"date\", \"email\",\n",
        "    \"pii_amount\", \"pii_card\", \"pii_date\", \"pii_email\",\n",
        "    # базовые доменные токены, чтобы не появлялись в названиях\n",
        "    \"банк\",\"банка\",\"банку\",\"банком\",\"банке\",\"банков\",\"банкам\",\"банках\",\"банками\",\n",
        "    \"клиент\",\"клиента\",\"клиенту\",\"клиентом\",\"клиенты\",\"клиентам\",\"клиентов\",\"клиентами\",\"клиентах\",\n",
        "    # вежливости/мета\n",
        "    \"добрый\",\"день\",\"здравствуйте\",\"пожалуйста\",\"коллеги\",\"уважаемый\",\"уважаемая\",\"уважаемые\",\n",
        "    \"просьба\",\"просит\",\"обращение\",\"ответ\",\n",
        "}\n",
        "\n",
        "# Русские стоп-слова (жёстко зашитый список без NLTK) + доменные слова\n",
        "RU_STOPWORDS = {\n",
        "    \"и\",\"в\",\"во\",\"не\",\"что\",\"он\",\"на\",\"я\",\"с\",\"со\",\"как\",\"а\",\"то\",\"все\",\"она\",\"так\",\"его\",\"но\",\"да\",\"ты\",\"к\",\"у\",\"же\",\"вы\",\"за\",\"бы\",\"по\",\"только\",\"ее\",\"мне\",\"было\",\"вот\",\"от\",\"меня\",\"еще\",\"нет\",\"о\",\"из\",\"ему\",\"теперь\",\"когда\",\"даже\",\"ну\",\"вдруг\",\"ли\",\"если\",\"уже\",\"или\",\"ни\",\"быть\",\"был\",\"него\",\"до\",\"вас\",\"нибудь\",\"опять\",\"уж\",\"вам\",\"ведь\",\"там\",\"потом\",\"себя\",\"ничего\",\"ей\",\"может\",\"они\",\"тут\",\"где\",\"есть\",\"надо\",\"ней\",\"для\",\"мы\",\"тебя\",\"их\",\"чем\",\"была\",\"сам\",\"чтоб\",\"без\",\"будто\",\"чего\",\"раз\",\"тоже\",\"себе\",\"под\",\"будет\",\"ж\",\"тогда\",\"кто\",\"этот\",\"того\",\"потому\",\"этого\",\"какой\",\"совсем\",\"ним\",\"здесь\",\"этом\",\"один\",\"почти\",\"мой\",\"тем\",\"чтобы\",\"нее\",\"сейчас\",\"были\",\"куда\",\"зачем\",\"всех\",\"никогда\",\"можно\",\"при\",\"наконец\",\"два\",\"об\",\"другой\",\"хоть\",\"после\",\"над\",\"больше\",\"тот\",\"через\",\"эти\",\"нас\",\"про\",\"всего\",\"них\",\"какая\",\"много\",\"разве\",\"три\",\"эту\",\"моя\",\"впрочем\",\"хорошо\",\"свою\",\"этой\",\"перед\",\"иногда\",\"лучше\",\"чуть\",\"том\",\"нельзя\",\"такой\",\"им\",\"более\",\"всегда\",\"конечно\",\"всю\",\"между\"\n",
        "}\n",
        "\n",
        "EXTRA_STOPWORDS = {\n",
        "    # вежливости/мета\n",
        "    \"добрый\",\"день\",\"здравствуйте\",\"пожалуйста\",\"коллеги\",\"уважаемый\",\"уважаемая\",\"уважаемые\",\n",
        "    # банк/клиент (частые формы)\n",
        "    \"клиент\",\"клиента\",\"клиенту\",\"клиентом\",\"клиенты\",\"клиентам\",\"клиентов\",\"клиентами\",\"клиентах\",\n",
        "    \"банк\",\"банка\",\"банку\",\"банком\",\"банке\",\"банков\",\"банкам\",\"банках\",\"банками\",\n",
        "    # мета-лексема\n",
        "    \"просьба\",\"просит\",\"обращение\",\"ответ\"\n",
        "}\n",
        "\n",
        "# Один объединённый список стоп-слов (list), совместимый со sklearn\n",
        "STOP_WORDS = sorted(RU_STOPWORDS | EXTRA_STOPWORDS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "df = pd.read_csv('output - 2025-10-29T194929.261.csv', encoding='utf-16', sep='(t')\n",
        "# Фильтрация по типу заявления (поле гарантированно есть)\n",
        "allowed_types = {\"Жалоба\", \"Претензия\", \"Предложение\"}\n",
        "df = df[df['Тип заявления'].astype(str).isin(allowed_types)].copy()\n",
        "# Пересобираем тексты после фильтрации\n",
        "texts = df['Описание претензии'].astype(str).fillna(\"\").tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Очистка текста\n",
        "\n",
        "PII_EMAIL = re.compile(r\"\\S+@\\S+\")\n",
        "PII_CARD = re.compile(r\"\\b\\d{12,16}\\b\")\n",
        "PII_DATE = re.compile(r\"\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\")\n",
        "PII_AMOUNT = re.compile(r\"\\d+[ ,\\.]?\\d*\\s?(₽|р\\.?|rub|\\$|usd|eur)?\", re.IGNORECASE)\n",
        "PII_PHONE = re.compile(r\"(\\+7|8)[\\s\\-\\(\\)]?\\d[\\d\\s\\-\\(\\)]{8,14}\")\n",
        "PII_PASSPORT = re.compile(r\"(паспорт|серия|сер\\.)[^\\n\\r]*?(\\d{2,4})[^\\n\\r]*?(номер|№|n)?[^\\n\\r]*?(\\d{6})\", re.IGNORECASE)\n",
        "PII_DOB_FIELD = re.compile(r\"(дата\\s+рождения\\s*:?)[^\\n\\r]*\", re.IGNORECASE)\n",
        "PII_FIO_FIELD = re.compile(r\"(фамилия[,\\s]*имя[,\\s]*отчество\\s*:?)[^\\n\\r]*\", re.IGNORECASE)\n",
        "PII_ADDRESS_FIELD = re.compile(r\"(адрес(\\s+постоянной\\s+регистрации)?\\s*:?)[^\\n\\r]*\", re.IGNORECASE)\n",
        "PII_PHONE_FIELD = re.compile(r\"(контактный\\s+телефон\\s*:?)[^\\n\\r]*\", re.IGNORECASE)\n",
        "\n",
        "# Удаляем URL и строки вида \"Адрес отзыва: ...\"\n",
        "RE_URL = re.compile(r\"https?://\\S+\", re.IGNORECASE)\n",
        "RE_REVIEW_ADDR_LINE = re.compile(r\"адрес\\s+отзыва:.*\", re.IGNORECASE)\n",
        "\n",
        "# Удаляем почтовые заголовки и служебные поля\n",
        "RE_MAIL_HEADERS = re.compile(r\"^(кому|копия|тема|получатель|от|cc|bcc|файл\\s+обращения|контакт\\-?центр)\\b.*\", re.IGNORECASE | re.MULTILINE)\n",
        "# Удаляем служебные мета-уведомления\n",
        "RE_META_NOTICES = re.compile(r\"^(пользователем\\s+.*?были\\s+запрошены|в\\s+\\\"?народный\\s+рейтинг\\\"?.*добавлен.*отзыв|заголовок:|оценка:|автор:|дата:|пункт\\s+обслуживания:|дополнительная\\s+информация).*\", re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "# Приветствия/шапки\n",
        "RE_GREETINGS = re.compile(r\"^(д\\.д\\.|дд\\!?|добрый\\s+день\\!?)[\\s,]*\", re.IGNORECASE)\n",
        "RE_META_PHRASES = re.compile(r\"\\b(со\\s+слов\\s+клиента|клиент\\s+пишет)\\b[:,\\s]*\", re.IGNORECASE)\n",
        "\n",
        "# Входящий документ (мета-записи)\n",
        "RE_INCOMING_DOC_LINE = re.compile(r\"^входящий\\s+документ\\s*(№|n|no)?\\s*\\d+\\s*от\\s*<date>.*$\", re.IGNORECASE | re.MULTILINE)\n",
        "RE_INCOMING_DOC_RAW = re.compile(r\"^входящий\\s+документ\\b.*\", re.IGNORECASE)\n",
        "\n",
        "RE_MULTISPACE = re.compile(r\"\\s{2,}\")\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Расширенная очистка: удаление служебных строк, URL, замена PII/телефонов/паспортов, упрощение шапок.\"\"\"\n",
        "    t = str(text).lower()\n",
        "    # удалить целиком строки \"Адрес отзыва: ...\", почтовые заголовки и мета-уведомления\n",
        "    t = RE_REVIEW_ADDR_LINE.sub(\" \", t)\n",
        "    t = RE_MAIL_HEADERS.sub(\" \", t)\n",
        "    t = RE_META_NOTICES.sub(\" \", t)\n",
        "    # удалить URL\n",
        "    t = RE_URL.sub(\" \", t)\n",
        "    # привести/срезать шапки типа ДД/добрый день и мета-фразы\n",
        "    t = RE_GREETINGS.sub(\" \", t)\n",
        "    t = RE_META_PHRASES.sub(\" \", t)\n",
        "    # PII замены\n",
        "    t = PII_EMAIL.sub(\"<email>\", t)\n",
        "    t = PII_CARD.sub(\"<card>\", t)\n",
        "    t = PII_DATE.sub(\"<date>\", t)\n",
        "    t = PII_AMOUNT.sub(\"<amount>\", t)\n",
        "    t = PII_PHONE.sub(\"<phone>\", t)\n",
        "    # Срез полей с персональными данными (оставляем маркеры, убираем значения)\n",
        "    t = PII_DOB_FIELD.sub(\"дата рождения: <date>\", t)\n",
        "    t = PII_FIO_FIELD.sub(\"фио: <name>\", t)\n",
        "    t = PII_ADDRESS_FIELD.sub(\"адрес: <address>\", t)\n",
        "    t = PII_PHONE_FIELD.sub(\"контактный телефон: <phone>\", t)\n",
        "    # Паспорт (серия/номер)\n",
        "    t = PII_PASSPORT.sub(\" <passport> \", t)\n",
        "    # нормализация пробелов\n",
        "    t = RE_MULTISPACE.sub(\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "\n",
        "def clean_corpus(texts: List[str]) -> List[str]:\n",
        "    return [clean_text(t) for t in texts]\n",
        "\n",
        "\n",
        "def should_drop_text(raw_text: str) -> bool:\n",
        "    \"\"\"Определить мета-записи без содержимого (например, только \"Входящий документ № ... от ...\").\"\"\"\n",
        "    t = str(raw_text).lower().strip()\n",
        "    if not t:\n",
        "        return True\n",
        "    if RE_INCOMING_DOC_RAW.match(t):\n",
        "        # уберем числа/даты/знаки и проверим остаток\n",
        "        tmp = PII_DATE.sub(\" \", t)\n",
        "        tmp = re.sub(r\"[№nno\\-:;.,\\s\\d]+\", \" \", tmp)\n",
        "        tmp = RE_MULTISPACE.sub(\" \", tmp).strip()\n",
        "        return len(tmp) < 8  # почти пустая мета-строка\n",
        "    return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Фильтрация мета-записей после очистки (опционально)\n",
        "# Удалим строки, которые состоят из служебного \"входящий документ ...\" без сути\n",
        "cleaned_texts = clean_corpus(texts)\n",
        "mask_keep = [not should_drop_text(t) for t in texts]\n",
        "texts = [t for t, keep in zip(cleaned_texts, mask_keep) if keep]\n",
        "print(f\"Оставлено документов: {len(texts)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пайплайн LDA\n",
        "\n",
        "class LdaTopicModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_topics: int = 10,\n",
        "        max_df: float = 0.95,\n",
        "        min_df: int = 5,\n",
        "        ngram_range: Tuple[int, int] = (1, 2),\n",
        "        random_state: int = 42,\n",
        "    ) -> None:\n",
        "        self.n_topics = n_topics\n",
        "        self.vectorizer = CountVectorizer(\n",
        "            stop_words=STOP_WORDS,\n",
        "            max_df=max_df,\n",
        "            min_df=min_df,\n",
        "            ngram_range=ngram_range,\n",
        "        )\n",
        "        self.lda = LatentDirichletAllocation(\n",
        "            n_components=n_topics,\n",
        "            learning_method=\"batch\",\n",
        "            max_iter=20,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, texts: List[str]) -> \"LdaTopicModel\":\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X = self.vectorizer.fit_transform(cleaned)\n",
        "        self.doc_topic = self.lda.fit_transform(X)\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X = self.vectorizer.transform(cleaned)\n",
        "        return self.lda.transform(X)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        dist = self.transform(texts)\n",
        "        return dist.argmax(axis=1), dist.max(axis=1)\n",
        "\n",
        "    def get_topic_top_words(self, top_n: int = 10) -> Dict[int, List[str]]:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "        topics: Dict[int, List[str]] = {}\n",
        "        for i, comp in enumerate(self.lda.components_):\n",
        "            top_idx = comp.argsort()[::-1]\n",
        "            words: List[str] = []\n",
        "            for j in top_idx:\n",
        "                w = feature_names[j]\n",
        "                if w in BLOCKED_TERMS:\n",
        "                    continue\n",
        "                words.append(w)\n",
        "                if len(words) >= top_n:\n",
        "                    break\n",
        "            topics[i] = words\n",
        "        return topics\n",
        "\n",
        "    def to_dataframe(self, texts: List[str]) -> pd.DataFrame:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        preds, conf = self.predict(texts)\n",
        "        return pd.DataFrame({\"text\": texts, \"topic_id\": preds, \"topic_confidence\": conf})\n",
        "\n",
        "    def perplexity(self, texts: Optional[List[str]] = None) -> float:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        if texts is None:\n",
        "            texts = self._last_fit_corpus if hasattr(self, \"_last_fit_corpus\") else None\n",
        "        cleaned = clean_corpus(texts) if texts is not None else None\n",
        "        X = self.vectorizer.transform(cleaned) if cleaned is not None else None\n",
        "        if X is None:\n",
        "            raise ValueError(\"Передайте texts для оценки перплексии\")\n",
        "        return float(self.lda.perplexity(X))\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        joblib.dump(self.vectorizer, os.path.join(path, \"vectorizer.joblib\"))\n",
        "        joblib.dump(self.lda, os.path.join(path, \"lda.joblib\"))\n",
        "        meta = {\"n_topics\": self.n_topics}\n",
        "        with open(os.path.join(path, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"LdaTopicModel\":\n",
        "        with open(os.path.join(path, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "        obj = cls(n_topics=meta[\"n_topics\"])\n",
        "        obj.vectorizer = joblib.load(os.path.join(path, \"vectorizer.joblib\"))\n",
        "        obj.lda = joblib.load(os.path.join(path, \"lda.joblib\"))\n",
        "        obj.is_fitted = True\n",
        "        return obj\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пайплайн LSI (TF-IDF + SVD) + KMeans\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class LsiKMeansTopicModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_components: int = 100,\n",
        "        n_clusters: int = 10,\n",
        "        max_df: float = 0.95,\n",
        "        min_df: int = 5,\n",
        "        ngram_range: Tuple[int, int] = (1, 2),\n",
        "        random_state: int = 42,\n",
        "    ) -> None:\n",
        "        self.n_components = n_components\n",
        "        self.n_clusters = n_clusters\n",
        "        self.tfidf = TfidfVectorizer(\n",
        "            stop_words=STOP_WORDS,\n",
        "            max_df=max_df,\n",
        "            min_df=min_df,\n",
        "            ngram_range=ngram_range,\n",
        "        )\n",
        "        self.svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
        "        self.lsi = make_pipeline(self.svd, Normalizer(copy=False))\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, texts: List[str]) -> \"LsiKMeansTopicModel\":\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X_tfidf = self.tfidf.fit_transform(cleaned)\n",
        "        X_lsi = self.lsi.fit_transform(X_tfidf)\n",
        "        self.labels_ = self.kmeans.fit_predict(X_lsi)\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        X_tfidf = self.tfidf.transform(cleaned)\n",
        "        return self.lsi.transform(X_tfidf)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> np.ndarray:\n",
        "        X_lsi = self.transform(texts)\n",
        "        return self.kmeans.predict(X_lsi)\n",
        "\n",
        "    def to_dataframe(self, texts: List[str]) -> pd.DataFrame:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        labels = self.predict(texts)\n",
        "        return pd.DataFrame({\"text\": texts, \"topic_id\": labels})\n",
        "\n",
        "    def silhouette(self, texts: List[str]) -> float:\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        X_lsi = self.transform(texts)\n",
        "        return float(silhouette_score(X_lsi, self.kmeans.predict(X_lsi)))\n",
        "\n",
        "    def get_cluster_terms(self, texts: List[str], top_n: int = 10) -> Dict[int, List[str]]:\n",
        "        \"\"\"c-TF-IDF по агрегированным документам кластеров.\n",
        "        Передайте исходные тексты корпуса (те же, на которых обучали).\n",
        "        \"\"\"\n",
        "        assert self.is_fitted, \"Сначала вызовите fit()\"\n",
        "        cleaned = clean_corpus(texts)\n",
        "        # Аггрегируем по кластерам\n",
        "        cluster_docs: Dict[int, List[str]] = defaultdict(list)\n",
        "        for t, lab in zip(cleaned, self.labels_):\n",
        "            cluster_docs[lab].append(t)\n",
        "        agg_texts = [\" \".join(cluster_docs[i]) if i in cluster_docs else \"\" for i in range(self.n_clusters)]\n",
        "        # Считаем c-TF-IDF\n",
        "        ctfidf = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=(1, 2), min_df=2)\n",
        "        C = ctfidf.fit_transform(agg_texts)\n",
        "        feat = ctfidf.get_feature_names_out()\n",
        "        terms: Dict[int, List[str]] = {}\n",
        "        for i in range(self.n_clusters):\n",
        "            row = C[i]\n",
        "            scores = row.toarray().ravel()\n",
        "            order = scores.argsort()[::-1]\n",
        "            words: List[str] = []\n",
        "            for j in order:\n",
        "                w = str(feat[j])\n",
        "                if w in BLOCKED_TERMS:\n",
        "                    continue\n",
        "                words.append(w)\n",
        "                if len(words) >= top_n:\n",
        "                    break\n",
        "            terms[i] = words\n",
        "        return terms\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        joblib.dump(self.tfidf, os.path.join(path, \"tfidf.joblib\"))\n",
        "        joblib.dump(self.svd, os.path.join(path, \"svd.joblib\"))\n",
        "        joblib.dump(self.lsi, os.path.join(path, \"lsi_pipeline.joblib\"))\n",
        "        joblib.dump(self.kmeans, os.path.join(path, \"kmeans.joblib\"))\n",
        "        meta = {\"n_components\": self.n_components, \"n_clusters\": self.n_clusters}\n",
        "        with open(os.path.join(path, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"LsiKMeansTopicModel\":\n",
        "        with open(os.path.join(path, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "        obj = cls(n_components=meta[\"n_components\"], n_clusters=meta[\"n_clusters\"])\n",
        "        obj.tfidf = joblib.load(os.path.join(path, \"tfidf.joblib\"))\n",
        "        obj.svd = joblib.load(os.path.join(path, \"svd.joblib\"))\n",
        "        obj.lsi = joblib.load(os.path.join(path, \"lsi_pipeline.joblib\"))\n",
        "        obj.kmeans = joblib.load(os.path.join(path, \"kmeans.joblib\"))\n",
        "        obj.is_fitted = True\n",
        "        return obj\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подбор числа тем/кластеров\n",
        "\n",
        "def lda_try_topics(texts: List[str], topics_list: List[int]) -> pd.DataFrame:\n",
        "    \"\"\"Перебор числа тем для LDA и оценка перплексии (ниже — лучше).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    cv = CountVectorizer(stop_words=STOP_WORDS, max_df=0.95, min_df=5, ngram_range=(1, 2))\n",
        "    X = cv.fit_transform(cleaned)\n",
        "    rows = []\n",
        "    for k in topics_list:\n",
        "        lda = LatentDirichletAllocation(n_components=k, learning_method=\"batch\", max_iter=15, random_state=42, n_jobs=-1)\n",
        "        lda.fit(X)\n",
        "        perp = float(lda.perplexity(X))\n",
        "        rows.append({\"n_topics\": k, \"perplexity\": perp})\n",
        "    return pd.DataFrame(rows).sort_values(\"n_topics\")\n",
        "\n",
        "\n",
        "def lsi_kmeans_try_k(texts: List[str], k_list: List[int], n_components: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"Перебор k для KMeans на LSI-признаках и оценка silhouette (выше — лучше).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    tfidf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.95, min_df=5, ngram_range=(1, 2))\n",
        "    X_tfidf = tfidf.fit_transform(cleaned)\n",
        "    lsi = make_pipeline(TruncatedSVD(n_components=n_components, random_state=42), Normalizer(copy=False))\n",
        "    X_lsi = lsi.fit_transform(X_tfidf)\n",
        "    rows = []\n",
        "    for k in k_list:\n",
        "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
        "        labels = km.fit_predict(X_lsi)\n",
        "        sil = float(silhouette_score(X_lsi, labels))\n",
        "        rows.append({\"k\": k, \"silhouette\": sil})\n",
        "    return pd.DataFrame(rows).sort_values(\"k\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демонстрация на примерах\n",
        "\n",
        "example_texts = texts[50]\n",
        "\n",
        "# 1) LDA\n",
        "lda_model = LdaTopicModel(n_topics=10)\n",
        "lda_model.fit(example_texts)\n",
        "lda_topics_df = lda_model.to_dataframe(example_texts)\n",
        "print(\"Топ-слова по темам (LDA):\")\n",
        "print(pd.DataFrame.from_dict(lda_model.get_topic_top_words(top_n=8), orient=\"index\"))\n",
        "\n",
        "display(lda_topics_df)\n",
        "\n",
        "# 2) LSI+KMeans\n",
        "lsi_model = LsiKMeansTopicModel(n_components=100, n_clusters=10)\n",
        "lsi_model.fit(example_texts)\n",
        "lsi_topics_df = lsi_model.to_dataframe(example_texts)\n",
        "print(\"\\nТоп-термины по кластерам (LSI+KMeans):\")\n",
        "print(pd.DataFrame.from_dict(lsi_model.get_cluster_terms(example_texts, top_n=8), orient=\"index\"))\n",
        "\n",
        "display(lsi_topics_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Применение к новым жалобам + сохранение/загрузка\n",
        "\n",
        "new_texts = [\n",
        "    \"Прошу вернуть годовую комиссию, не было уведомления о списании\",\n",
        "    \"Перевод на казахстанскую карту не дошёл, сделайте розыск\",\n",
        "]\n",
        "\n",
        "lda_pred_ids, lda_pred_conf = lda_model.predict(new_texts)\n",
        "print(\"LDA предсказания:\")\n",
        "print(pd.DataFrame({\"text\": new_texts, \"topic_id\": lda_pred_ids, \"confidence\": lda_pred_conf}))\n",
        "\n",
        "lsi_pred_ids = lsi_model.predict(new_texts)\n",
        "print(\"\\nLSI+KMeans предсказания:\")\n",
        "print(pd.DataFrame({\"text\": new_texts, \"topic_id\": lsi_pred_ids}))\n",
        "\n",
        "# Сохранение\n",
        "lda_model.save(\"models/lda_model\")\n",
        "lsi_model.save(\"models/lsi_kmeans_model\")\n",
        "\n",
        "# Загрузка\n",
        "lda_loaded = LdaTopicModel.load(\"models/lda_model\")\n",
        "lsi_loaded = LsiKMeansTopicModel.load(\"models/lsi_kmeans_model\")\n",
        "\n",
        "_ = lda_loaded.predict([\"Возврат страховки по кредиту не произведён\"])  # smoke-test\n",
        "_ = lsi_loaded.predict([\"Карта заблокирована без предупреждения\"])  # smoke-test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Пост-обработка: мэппинг тем в бизнес‑рубрики\n",
        "\n",
        "- Создаем словарь соответствия `исходная_тема → бизнес‑тема`.\n",
        "- LDA: суммируем вероятности по темам, входящим в одну бизнес‑тему, и берём максимум.\n",
        "- Пересчитываем ключевые термины по финальным бизнес‑темам (c‑TF‑IDF).\n",
        "- Анализируем размер финальных тем (шт. и % от корпуса).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def apply_lda_mapping(lda_model: LdaTopicModel, texts: List[str], topic_to_business: Dict[int, str]) -> pd.DataFrame:\n",
        "    \"\"\"Возвращает финальную бизнес-метку и уверенность (сумма вероятностей) по документу.\"\"\"\n",
        "    dist = lda_model.transform(texts)\n",
        "    final_labels: List[str] = []\n",
        "    final_conf: List[float] = []\n",
        "    for row in dist:\n",
        "        agg: Dict[str, float] = {}\n",
        "        for topic_id, p in enumerate(row):\n",
        "            label = topic_to_business.get(topic_id, f\"topic_{topic_id}\")\n",
        "            agg[label] = agg.get(label, 0.0) + float(p)\n",
        "        best_label, best_prob = max(agg.items(), key=lambda x: x[1])\n",
        "        final_labels.append(best_label)\n",
        "        final_conf.append(best_prob)\n",
        "    return pd.DataFrame({\"text\": texts, \"final_label\": final_labels, \"final_conf\": final_conf})\n",
        "\n",
        "\n",
        "def apply_kmeans_mapping(lsi_model: LsiKMeansTopicModel, texts: List[str], cluster_to_business: Dict[int, str]) -> pd.DataFrame:\n",
        "    labels = lsi_model.predict(texts)\n",
        "    final_labels = [cluster_to_business.get(int(l), f\"cluster_{int(l)}\") for l in labels]\n",
        "    return pd.DataFrame({\"text\": texts, \"final_label\": final_labels})\n",
        "\n",
        "\n",
        "def compute_business_terms(texts: List[str], labels: List[str], top_n: int = 10) -> Dict[str, List[str]]:\n",
        "    \"\"\"Считает ключевые термины для финальных бизнес‑тем (c‑TF‑IDF).\"\"\"\n",
        "    cleaned = clean_corpus(texts)\n",
        "    groups: Dict[str, List[str]] = {}\n",
        "    for t, lab in zip(cleaned, labels):\n",
        "        groups.setdefault(lab, []).append(t)\n",
        "    business_names = sorted(groups.keys())\n",
        "    agg_texts = [\" \".join(groups[name]) for name in business_names]\n",
        "    vec = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=(1, 2), min_df=2)\n",
        "    M = vec.fit_transform(agg_texts)\n",
        "    feat = vec.get_feature_names_out()\n",
        "    terms: Dict[str, List[str]] = {}\n",
        "    for idx, name in enumerate(business_names):\n",
        "        row = M[idx]\n",
        "        scores = row.toarray().ravel()\n",
        "        order = scores.argsort()[::-1]\n",
        "        words: List[str] = []\n",
        "        for j in order:\n",
        "            w = str(feat[j])\n",
        "            if w in BLOCKED_TERMS:\n",
        "                continue\n",
        "            words.append(w)\n",
        "            if len(words) >= top_n:\n",
        "                break\n",
        "        terms[name] = words\n",
        "    return terms\n",
        "\n",
        "\n",
        "def business_size_report(labels: List[str]) -> pd.DataFrame:\n",
        "    cnt = Counter(labels)\n",
        "    total = sum(cnt.values())\n",
        "    rows = [{\"business_label\": k, \"count\": v, \"share\": v / total} for k, v in sorted(cnt.items(), key=lambda x: -x[1])]\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Пример использования (заполните mapping под ваши рубрики)\n",
        "# Мэппинг для LDA: исходная_тема -> бизнес‑тема\n",
        "lda_mapping_example = {i: f\"Тема_{i}\" for i in range(lda_model.lda.n_components)}\n",
        "lda_final_df = apply_lda_mapping(lda_model, example_texts, lda_mapping_example)\n",
        "lda_terms_final = compute_business_terms(example_texts, lda_final_df[\"final_label\"].tolist(), top_n=8)\n",
        "print(\"Финальные бизнес‑темы (LDA):\")\n",
        "print(business_size_report(lda_final_df[\"final_label\"].tolist()))\n",
        "print(pd.DataFrame.from_dict(lda_terms_final, orient=\"index\"))\n",
        "\n",
        "# Мэппинг для LSI+KMeans: кластер -> бизнес‑тема\n",
        "kmeans_mapping_example = {i: f\"Тема_{i}\" for i in range(lsi_model.n_clusters)}\n",
        "kmeans_final_df = apply_kmeans_mapping(lsi_model, example_texts, kmeans_mapping_example)\n",
        "kmeans_terms_final = compute_business_terms(example_texts, kmeans_final_df[\"final_label\"].tolist(), top_n=8)\n",
        "print(\"\\nФинальные бизнес‑темы (LSI+KMeans):\")\n",
        "print(business_size_report(kmeans_final_df[\"final_label\"].tolist()))\n",
        "print(pd.DataFrame.from_dict(kmeans_terms_final, orient=\"index\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Автонаименование тем из n‑грамм (1–3 слов)\n",
        "\n",
        "Алгоритм: считаем c‑TF‑IDF по агрегатам тем, генерируем кандидатов (уни/би/триграммы), бустим фразы длиннее, фильтруем служебные и дубликаты‑включения, выбираем 1–2 лучших названий динамической длины.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "\n",
        "def _is_blocked_phrase(phrase: str) -> bool:\n",
        "    # блокируем, если вся фраза — плейсхолдер или если содержит чисто служебные токены\n",
        "    tokens = phrase.split()\n",
        "    if phrase in BLOCKED_TERMS:\n",
        "        return True\n",
        "    if any(tok in BLOCKED_TERMS for tok in tokens):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def generate_labels_for_groups(\n",
        "    groups: Dict[int, List[str]],\n",
        "    ngram_range: Tuple[int, int] = (1, 3),\n",
        "    min_df: int = 2,\n",
        "    top_n_candidates: int = 50,\n",
        "    phrase_boost: Tuple[float, float, float] = (1.0, 1.1, 1.2),\n",
        "    inclusion_margin: float = 0.15,\n",
        "    final_top_k: int = 1,\n",
        ") -> Dict[int, List[str]]:\n",
        "    \"\"\"\n",
        "    Генерирует динамические названия тем по агрегированным группам документов.\n",
        "    - phrase_boost: множители для уни/би/три-gram.\n",
        "    - inclusion_margin: если более длинная фраза включает короткую и их скора близки (<15%), оставляем длинную.\n",
        "    - final_top_k: сколько ярлыков вернуть на тему.\n",
        "    \"\"\"\n",
        "    # агрегация\n",
        "    group_ids = sorted(groups.keys())\n",
        "    agg_texts = [\" \".join(groups[i]) for i in group_ids]\n",
        "\n",
        "    vec = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=ngram_range, min_df=min_df)\n",
        "    M = vec.fit_transform(agg_texts)\n",
        "    feat = vec.get_feature_names_out()\n",
        "\n",
        "    def gram_len(term: str) -> int:\n",
        "        return len(term.split())\n",
        "\n",
        "    labels: Dict[int, List[str]] = {}\n",
        "    for row_idx, gid in enumerate(group_ids):\n",
        "        scores = M[row_idx].toarray().ravel()\n",
        "        order = scores.argsort()[::-1]\n",
        "        candidates: List[Tuple[str, float]] = []\n",
        "        for j in order[: top_n_candidates * 3]:  # с запасом\n",
        "            term = str(feat[j])\n",
        "            if _is_blocked_phrase(term):\n",
        "                continue\n",
        "            gl = gram_len(term)\n",
        "            if gl == 1:\n",
        "                adj = scores[j] * phrase_boost[0]\n",
        "            elif gl == 2:\n",
        "                adj = scores[j] * phrase_boost[1]\n",
        "            else:\n",
        "                adj = scores[j] * phrase_boost[2]\n",
        "            candidates.append((term, adj))\n",
        "            if len(candidates) >= top_n_candidates:\n",
        "                break\n",
        "        # устранение включений: предпочитаем более длинные, если близкие по скору\n",
        "        final: List[Tuple[str, float]] = []\n",
        "        for term, sc in candidates:\n",
        "            drop = False\n",
        "            # если уже взята более длинная фраза, содержащая текущую, и её скор не существенно хуже\n",
        "            for k, (t2, sc2) in enumerate(list(final)):\n",
        "                if t2 in term and sc >= sc2 * (1 - inclusion_margin):\n",
        "                    # текущая длиннее и не хуже — заменим\n",
        "                    final[k] = (term, sc)\n",
        "                    drop = True\n",
        "                    break\n",
        "                if term in t2 and sc2 >= sc * (1 - inclusion_margin):\n",
        "                    # уже есть более длинная похожая — отбрасываем\n",
        "                    drop = True\n",
        "                    break\n",
        "            if not drop:\n",
        "                final.append((term, sc))\n",
        "        final_sorted = [t for t, _ in sorted(final, key=lambda x: x[1], reverse=True)]\n",
        "        labels[gid] = final_sorted[:final_top_k]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def generate_lda_labels(\n",
        "    lda_model: LdaTopicModel,\n",
        "    texts: List[str],\n",
        "    final_top_k: int = 1,\n",
        "    ngram_range: Tuple[int, int] = (1, 3),\n",
        "    min_df: int = 2,\n",
        ") -> Dict[int, List[str]]:\n",
        "    # назначаем документ в тему по argmax\n",
        "    dist = lda_model.transform(texts)\n",
        "    assigned = dist.argmax(axis=1)\n",
        "    groups: Dict[int, List[str]] = {}\n",
        "    for t, lab in zip(texts, assigned):\n",
        "        groups.setdefault(int(lab), []).append(t)\n",
        "    return generate_labels_for_groups(groups, ngram_range=ngram_range, min_df=min_df, final_top_k=final_top_k)\n",
        "\n",
        "\n",
        "def generate_kmeans_labels(\n",
        "    lsi_model: LsiKMeansTopicModel,\n",
        "    texts: List[str],\n",
        "    final_top_k: int = 1,\n",
        "    ngram_range: Tuple[int, int] = (1, 3),\n",
        "    min_df: int = 2,\n",
        ") -> Dict[int, List[str]]:\n",
        "    labels = lsi_model.predict(texts)\n",
        "    groups: Dict[int, List[str]] = {}\n",
        "    for t, lab in zip(texts, labels):\n",
        "        groups.setdefault(int(lab), []).append(t)\n",
        "    return generate_labels_for_groups(groups, ngram_range=ngram_range, min_df=min_df, final_top_k=final_top_k)\n",
        "\n",
        "# Пример: автоназвания на примерах\n",
        "lda_auto = generate_lda_labels(lda_model, example_texts, final_top_k=1, ngram_range=(1,3), min_df=1)\n",
        "print(\"LDA автоназвания тем:\", lda_auto)\n",
        "\n",
        "kmeans_auto = generate_kmeans_labels(lsi_model, example_texts, final_top_k=1, ngram_range=(1,3), min_df=1)\n",
        "print(\"KMeans автоназвания тем:\", kmeans_auto)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA: полный запуск (обучение → топ-термины → автоназвания → таблица)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение LDA\n",
        "lda_model = LdaTopicModel(n_topics=12, max_df=1.0, min_df=2, ngram_range=(1,2))\n",
        "lda_model.fit(texts)\n",
        "\n",
        "# Топ-термины тем\n",
        "lda_top = pd.DataFrame.from_dict(lda_model.get_topic_top_words(top_n=10), orient=\"index\")\n",
        "display(lda_top)\n",
        "\n",
        "# Автоназвания тем из n-грамм (1–3)\n",
        "lda_auto = generate_lda_labels(lda_model, texts, final_top_k=1, ngram_range=(1,3), min_df=2)\n",
        "\n",
        "# Таблица документов с темой и ярлыком\n",
        "df_lda = lda_model.to_dataframe(texts)\n",
        "df_lda[\"auto_label\"] = df_lda[\"topic_id\"].map({tid: (labs[0] if labs else \"\") for tid, labs in lda_auto.items()})\n",
        "display(df_lda.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSI+KMeans: полный запуск (обучение → термины → автоназвания → таблица)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение LSI+KMeans\n",
        "# Подберите n_components под размер корпуса; 50 — безопасная отправная точка\n",
        "lsi_model = LsiKMeansTopicModel(n_components=50, n_clusters=12, max_df=1.0, min_df=2, ngram_range=(1,2))\n",
        "lsi_model.fit(texts)\n",
        "\n",
        "# Топ-термины кластеров\n",
        "lsi_terms = pd.DataFrame.from_dict(lsi_model.get_cluster_terms(texts, top_n=10), orient=\"index\")\n",
        "display(lsi_terms)\n",
        "\n",
        "# Автоназвания кластеров из n-грамм (1–3)\n",
        "kmeans_auto = generate_kmeans_labels(lsi_model, texts, final_top_k=1, ngram_range=(1,3), min_df=2)\n",
        "\n",
        "# Таблица документов с кластером и ярлыком\n",
        "df_lsi = lsi_model.to_dataframe(texts)\n",
        "df_lsi[\"auto_label\"] = df_lsi[\"topic_id\"].map({cid: (labs[0] if labs else \"\") for cid, labs in kmeans_auto.items()})\n",
        "display(df_lsi.head())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
