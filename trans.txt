"""
Отбор признаков для регрессии: важность из дерева и RFE (без кросс-валидации).
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.feature_selection import RFE
from sklearn.metrics import mean_squared_error, mean_absolute_error


def select_by_importance(data, target_col, eval_data, max_features=None, importance_threshold=None, random_state=42):
    """
    Важность из LGBMRegressor. Возвращает (selected, imp, model) — model для evaluate_model.
    """
    X = data.drop(columns=[target_col])
    y = data[target_col]
    eval_set = [(eval_data.drop(columns=[target_col]), eval_data[target_col])]
    X_ = X.select_dtypes(include=[np.number]).copy()
    eval_align = [(X_val[X_.columns], y_val) for X_val, y_val in eval_set]
    m = lgb.LGBMRegressor(random_state=random_state, n_estimators=100, max_depth=4, verbosity=-1)
    m.fit(X_, y, eval_set=eval_align)
    imp = pd.Series(m.feature_importances_, index=X_.columns).sort_values(ascending=False)
    selected = imp[imp >= importance_threshold].index.tolist() if importance_threshold is not None else imp.index.tolist()
    if max_features is not None:
        selected = imp.head(max_features).index.tolist()
    return selected, imp, m


def select_by_rfe(data, target_col, estimator=None, n_features_to_select=100, step=50):
    """
    RFE без кросс-валидации: рекурсивное удаление признаков до n_features_to_select.
    Возвращает (selected, selector, feature_names, model): model — обученная модель (selector.estimator_) для evaluate_model.
    """
    X = data.drop(columns=[target_col])
    y = data[target_col]
    X_ = X.select_dtypes(include=[np.number]).copy()
    if estimator is None:
        estimator = lgb.LGBMRegressor(n_estimators=50, max_depth=4, random_state=42, verbosity=-1)
    selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=step)
    selector.fit(X_, y)
    return X_.columns[selector.support_].tolist(), selector, X_.columns.tolist(), selector.estimator_


def evaluate_model(model, train_df, test_df, oot_df, target_col, type_="cnt", avg_3m_col=None):
    """
    Сводка по модели: Train / Test / OOT. Возвращает DataFrame с колонками:
    Target, AVG target, AVG pred, AVG 3m, Dataset, MAPE, MAE, MAE 95%, RMSE, TYPE, Non Zero.
    type_ — 'cnt' или 'sum'. avg_3m_col — имя колонки для «AVG 3m» (среднее по датасету); если None — 0.
    """
    feat = getattr(model, "feature_name_", None) or getattr(model, "feature_names_in_", None)
    if feat is None:
        feat = [c for c in train_df.columns if c != target_col]
    rows = []
    for name, df in [("Train", train_df), ("Test", test_df), ("OOT", oot_df)]:
        X = df[feat] if all(c in df.columns for c in feat) else df.drop(columns=[target_col])
        y = df[target_col].values
        pred = model.predict(X)
        nz = (y != 0)
        mape = np.mean(np.abs((y - pred)[nz]) / y[nz]) if nz.sum() > 0 else np.nan
        mae = np.mean(np.abs(y - pred))
        mae_95 = np.percentile(np.abs(y - pred), 95)
        rmse = np.sqrt(np.mean((y - pred) ** 2))
        avg_3m = df[avg_3m_col].mean() if avg_3m_col and avg_3m_col in df.columns else 0.0
        rows.append({
            "Target": target_col,
            "AVG target": y.mean(),
            "AVG pred": pred.mean(),
            "AVG 3m": avg_3m,
            "Dataset": name,
            "MAPE": mape,
            "MAE": mae,
            "MAE 95%": mae_95,
            "RMSE": rmse,
            "TYPE": type_,
            "Non Zero": int(nz.sum()),
        })
    return pd.DataFrame(rows)


def metric_vs_n_features(data, target_col, eval_data, imp, step=30, metric="rmse", random_state=42):
    """
    Перебор числа признаков по важности: от 20 с шагом step до n_all. imp — результат select_by_importance (второй возврат).
    Возвращает (report_df, best_n, selected, model).
    """
    MIN_FEATURES = 20
    n_all = len(imp)
    start = min(MIN_FEATURES, n_all)
    n_features_list = list(range(start, n_all + 1, step))
    if n_all not in n_features_list:
        n_features_list.append(n_all)
    n_features_list = sorted(set(n_features_list))

    X_ev = eval_data[imp.index.tolist()]
    y_ev = eval_data[target_col].values
    fn = mean_squared_error if metric == "rmse" else mean_absolute_error

    rows = []
    for n in n_features_list:
        cols = imp.head(n).index.tolist()
        X_tr = data[cols]
        y_tr = data[target_col]
        m = lgb.LGBMRegressor(random_state=random_state, n_estimators=100, max_depth=4, verbosity=-1)
        m.fit(X_tr, y_tr, eval_set=[(X_ev[cols], y_ev)])
        pred = m.predict(X_ev[cols])
        val = np.sqrt(fn(y_ev, pred)) if metric == "rmse" else fn(y_ev, pred)
        rows.append({"n_features": n, "metric_value": val})

    report_df = pd.DataFrame(rows)
    best_n = int(report_df.loc[report_df["metric_value"].idxmin(), "n_features"])
    selected = imp.head(best_n).index.tolist()
    model = lgb.LGBMRegressor(random_state=random_state, n_estimators=100, max_depth=4, verbosity=-1)
    model.fit(data[selected], data[target_col], eval_set=[(eval_data[selected], eval_data[target_col])])

    plt.figure(figsize=(8, 4))
    plt.plot(report_df["n_features"], report_df["metric_value"], marker="o")
    plt.axvline(best_n, color="gray", linestyle="--", label=f"best n={best_n}")
    plt.xlabel("Число признаков")
    plt.ylabel(metric.upper())
    plt.title("Метрика на eval от n (топ по важности)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    return report_df, best_n, selected, model


selected, imp, _ = select_by_importance(train_df, target_col="target", eval_data=test_df, max_features=500)
report_n, best_n, selected, model = metric_vs_n_features(train_df[selected + ["target"]], target_col="target", eval_data=test_df[selected + ["target"]], imp=imp, step=30)
