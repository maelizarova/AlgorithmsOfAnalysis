import pandas as pd
import numpy as np
from typing import Dict, List, Union, Any, Optional
from sklearn.base import BaseEstimator
from sklearn.metrics import get_scorer
from sklearn.preprocessing import OrdinalEncoder

def select_top_features_with_eval_set(
    data: pd.DataFrame,
    targets: List[str],
    model: BaseEstimator,
    eval_set: Optional[pd.DataFrame] = None,  # Передаём целиком датафрейм
    n_top: int = 10,
    categorical_features: Optional[List[str]] = None,
    preprocess_categorical: bool = True,
    metric: Optional[Union[str, callable]] = None,
    greater_is_better: Optional[bool] = None,
    early_stopping_rounds: Optional[int] = None,
    random_state: int = 42,
    verbose: bool = False,
    **fit_params: Any
) -> Dict[str, List[str]]:
    """
    Отбирает топ-N фичей для каждого таргета с поддержкой:
    - eval_set как датафрейма (разбивается автоматически)
    - Категориальных фичей
    - Ранней остановки (early_stopping_rounds)
    - Любых моделей (LightGBM, CatBoost, XGBoost, etc.)

    Параметры:
    ----------
    eval_set : pd.DataFrame, optional
        Валидационный датафрейм (должен содержать все фичи и таргеты из `data`).
    """
    top_features = {}
    
    for target in targets:
        # Подготовка данных
        X = data.drop(columns=targets)
        y = data[target]
        
        # Обработка категориальных фичей
        if categorical_features is None:
            categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if preprocess_categorical and categorical_features:
            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
            X[categorical_features] = encoder.fit_transform(X[categorical_features])
            
        # Подготовка eval_set (если передан)
        if eval_set is not None:
            X_valid = eval_set.drop(columns=targets)
            y_valid = eval_set[target]
            
            if preprocess_categorical and categorical_features:
                X_valid[categorical_features] = encoder.transform(X_valid[categorical_features])
            
            # Для моделей, поддерживающих eval_set
            if hasattr(model, 'fit'):
                fit_params['eval_set'] = [(X_valid, y_valid)]
                if early_stopping_rounds:
                    fit_params['early_stopping_rounds'] = early_stopping_rounds
        
        # Обучение модели
        model.set_params(random_state=random_state)
        if verbose:
            print(f"Обучение для таргета: {target}")
        
        model.fit(X, y, **fit_params)
        
        # Получение важности фичей
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
        elif metric is not None:
            from sklearn.inspection import permutation_importance
            scorer = get_scorer(metric) if isinstance(metric, str) else metric
            result = permutation_importance(model, X, y, scoring=scorer, n_repeats=5, random_state=random_state)
            importance = result.importances_mean
        else:
            raise ValueError("Model has no feature_importances_ and no metric provided.")
        
        # Топ-N фичей
        top_features[target] = pd.DataFrame({
            'feature': X.columns,
            'importance': importance
        }).sort_values('importance', ascending=False).head(n_top)['feature'].tolist()
    
    return top_features

from lightgbm import LGBMRegressor

model = LGBMRegressor(n_estimators=1000)
selected_features = select_top_features_with_eval_set(
    data=train_data,          # Тренировочные данные
    targets=targets,
    model=model,
    eval_set=eval_data,       # Передаём весь датафрейм
    categorical_features=["product_category", "region"],
    early_stopping_rounds=50,
    fit_params={"eval_metric": "rmse", "verbose": 10}
)

print(selected_features)
# {'sales': ['feature1', 'feature2', ...], 'revenue': ['feature3', ...]}
