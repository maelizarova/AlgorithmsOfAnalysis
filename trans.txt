import re
import json
import os
import numpy as np
from datetime import datetime

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —á–∏—Å–µ–ª –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
def clean_complaint_text(text):
    """
    –£–¥–∞–ª—è–µ—Ç –≤—Å–µ —á–∏—Å–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∂–∞–ª–æ–±—ã –¥–ª—è –∑–∞—â–∏—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    cleaned_text = re.sub(r'\d+', '', text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–∑–∞–≥–ª—É—à–∫–∞ - –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ API)
def get_embedding(text, model="embedding-model"):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ API
    """
    try:
        # –ó–î–ï–°–¨ –í–ê–® –†–ï–ê–õ–¨–ù–´–ô –í–´–ó–û–í API –î–õ–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–ê
        # response = call_gpt_oss_embedding(text)
        # embedding = response.embedding
        
        # –ó–∞–≥–ª—É—à–∫–∞ - —Å–ª—É—á–∞–π–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        embedding = np.random.randn(512).tolist()
        return embedding
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def save_results(results, batch_number, total_processed, output_dir="classification_results"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{output_dir}/batch_{batch_number}_{timestamp}.json"
    
    results_data = {
        "metadata": {
            "batch_number": batch_number,
            "total_processed": total_processed,
            "timestamp": timestamp,
            "saved_at": datetime.now().isoformat(),
            "embedding_dimension": len(results[0]['embedding']) if results and 'embedding' in results[0] else 0
        },
        "results": results
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"‚úì Batch {batch_number} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({total_processed} –∂–∞–ª–æ–±)")
    return filename

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def load_previous_results(output_dir="classification_results"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π batch –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
    """
    if not os.path.exists(output_dir):
        return None, 0
    
    result_files = [f for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.json')]
    if not result_files:
        return None, 0
    
    latest_file = sorted(result_files)[-1]
    latest_path = os.path.join(output_dir, latest_file)
    
    with open(latest_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_processed = data['metadata']['total_processed']
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {latest_file} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed})")
    
    return data, total_processed

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π
def load_knowledge_bases():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    with open('products_knowledge_base.json', 'r', encoding='utf-8') as f:
        products_kb = json.load(f)
    
    with open('complaint_categories_knowledge_base.json', 'r', encoding='utf-8') as f:
        complaint_categories_kb = json.load(f)
    
    return products_kb, complaint_categories_kb

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM
def create_prompt(products_kb, categories_kb, complaint_text):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∂–∞–ª–æ–±—ã
    """
    prompt_template = """
# –†–æ–ª—å
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –±–∞–Ω–∫–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –∂–∞–ª–æ–±—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.

# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π

## –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:
{products_json}

## –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –∏ –ø–æ–¥—Ç–∏–ø–æ–≤ –∂–∞–ª–æ–±:
{categories_json}

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:

1. **–ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê:** –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–π —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç.

2. **–í–´–ë–û–† –ü–†–û–î–£–ö–¢–ê:**
   - –í—ã–±–µ—Ä–∏ –û–î–ò–ù –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –ï—Å–ª–∏ –ø—Ä–æ–¥—É–∫—Ç –Ω–µ—è—Å–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù"
   - –û—Ä–∏–µ–Ω—Ç–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞

3. **–í–´–ë–û–† –¢–ò–ü–ê –ò –ü–û–î–¢–ò–ü–ê –ñ–ê–õ–û–ë–´:**
   - –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∂–∞–ª–æ–±—ã
   - –ó–∞—Ç–µ–º –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–æ–¥—Ç–∏–ø
   - –ò—Å–ø–æ–ª—å–∑—É–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ typical_keywords –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è

4. **–ü–†–ò–ù–¶–ò–ü–´ –í–´–ë–û–†–ê:**
   - –í—ã–±–∏—Ä–∞–π –Ω–∞–∏–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–¥—Ç–∏–ø, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
   - –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, –≤—ã–±–µ—Ä–∏ –±–ª–∏–∂–∞–π—à–∏–π –ø–æ–¥—Ç–∏–ø
   - –ï—Å–ª–∏ –∂–∞–ª–æ–±–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é, –∏—Å–ø–æ–ª—å–∑—É–π "–î–†–£–ì–û–ï"

# –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "product": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–ø—Ä–æ–¥—É–∫—Ç–∞",
  "complaint_type": "—Ç–∏–ø_–∂–∞–ª–æ–±—ã", 
  "complaint_subtype": "–ø–æ–¥—Ç–∏–ø_–∂–∞–ª–æ–±—ã",
  "confidence": "–≤—ã—Å–æ–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–Ω–∏–∑–∫–∞—è",
  "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞"
}}

# –ñ–∞–ª–æ–±–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:
"{complaint_text}"

# –¢–≤–æ–π –∞–Ω–∞–ª–∏–∑:
"""

    return prompt_template.format(
        products_json=json.dumps(products_kb, ensure_ascii=False, indent=2),
        categories_json=json.dumps(categories_kb, ensure_ascii=False, indent=2),
        complaint_text=complaint_text
    )

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
def process_complaints_with_autosave(complaints_list, batch_size=100, start_from=0, output_dir="classification_results"):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∂–∞–ª–æ–± —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–∂–¥—ã–µ batch_size –∂–∞–ª–æ–±
    """
    products_kb, categories_kb = load_knowledge_bases()
    
    previous_results, total_processed = load_previous_results(output_dir)
    if start_from > 0:
        total_processed = start_from
    
    results = []
    batch_number = total_processed // batch_size + 1
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(complaints_list)} –∂–∞–ª–æ–±")
    print(f"üì¶ Batch size: {batch_size}")
    print(f"‚è© –°—Ç–∞—Ä—Ç —Å –ø–æ–∑–∏—Ü–∏–∏: {total_processed}")
    print("‚îÄ" * 50)
    
    for i, complaint in enumerate(complaints_list[total_processed:], start=total_processed + 1):
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = clean_complaint_text(complaint)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        embedding = get_embedding(cleaned_text)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        individual_prompt = create_prompt(products_kb, categories_kb, cleaned_text)
        
        result_item = {
            'id': i,
            'original_text': complaint,
            'cleaned_text': cleaned_text,
            'embedding': embedding,
            'embedding_dimension': len(embedding) if embedding else 0,
            'prompt': individual_prompt,
            'classification': None,
            'processed_at': datetime.now().isoformat()
        }
        
        results.append(result_item)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 –∂–∞–ª–æ–±
        if i % 1000 == 0:
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(complaints_list)} ({i/len(complaints_list)*100:.1f}%)")
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ batch_size –∂–∞–ª–æ–±
        if i % batch_size == 0:
            batch_number = i // batch_size
            save_results(results, batch_number, i, output_dir)
            results = []
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if results:
        batch_number += 1
        save_results(results, batch_number, len(complaints_list), output_dir)
    
    print("‚îÄ" * 50)
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ: {len(complaints_list)} –∂–∞–ª–æ–±")
    
    create_summary_file(complaints_list, output_dir)

def create_summary_file(complaints_list, output_dir):
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    batch_files = [f for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.json')]
    total_embeddings = 0
    embedding_dimensions = set()
    
    for batch_file in batch_files:
        batch_path = os.path.join(output_dir, batch_file)
        with open(batch_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            total_embeddings += len(data['results'])
            if data['results']:
                embedding_dimensions.add(data['results'][0]['embedding_dimension'])
    
    summary = {
        "total_complaints": len(complaints_list),
        "total_with_embeddings": total_embeddings,
        "embedding_dimensions": list(embedding_dimensions),
        "processed_at": datetime.now().isoformat(),
        "output_directory": output_dir,
        "batches_count": len(batch_files)
    }
    
    summary_file = f"{output_dir}/processing_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"üìä –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_file}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö batch'–µ–π
def merge_all_results(output_dir="classification_results"):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ batch —Ñ–∞–π–ª—ã –≤ –æ–¥–∏–Ω"""
    if not os.path.exists(output_dir):
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    all_results = []
    batch_files = [f for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.json')]
    
    for batch_file in sorted(batch_files):
        batch_path = os.path.join(output_dir, batch_file)
        with open(batch_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_results.extend(data['results'])
    
    merged_file = f"{output_dir}/all_results_merged.json"
    with open(merged_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã: {merged_file}")
    print(f"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_results)}")
    print(f"üî§ –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {sum(1 for r in all_results if r.get('embedding'))}")
    
    return all_results

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
def load_embeddings_only(output_dir="classification_results"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    all_results = merge_all_results(output_dir)
    embeddings = []
    texts = []
    
    for result in all_results:
        if result.get('embedding'):
            embeddings.append(result['embedding'])
            texts.append(result['cleaned_text'])
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    return np.array(embeddings), texts

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä —Å–ø–∏—Å–∫–∞ –∂–∞–ª–æ–±
    complaints = [
        "–ú–Ω–µ –Ω–µ –æ–¥–æ–±—Ä–∏–ª–∏ –∏–ø–æ—Ç–µ–∫—É –ø–æ–¥ 8% –≥–æ–¥–æ–≤—ã—Ö, —Ö–æ—Ç—è –∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –∏–¥–µ–∞–ª—å–Ω–∞—è",
        "–°–æ —Å—á–µ—Ç–∞ —Å–ø–∏—Å–∞–ª–∏ 500 —Ä—É–±–ª–µ–π –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏, –æ –∫–æ—Ç–æ—Ä–æ–π –º–µ–Ω—è –Ω–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–ª–∏",
        "–ù–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å–º—Å —Å –∫–æ–¥–æ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ",
        "–ó–≤–æ–Ω—è—Ç –º–æ–∏–º —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞–º –∏ —Å–æ–æ–±—â–∞—é—Ç –æ –º–æ–µ–π –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç–µ 1234-5678-9012-3456"
    ] * 25
    
    # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–∂–¥—ã–µ 100 –∂–∞–ª–æ–±
    process_complaints_with_autosave(
        complaints_list=complaints,
        batch_size=100,
        start_from=0,
        output_dir="classification_results"
    )
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    all_results = merge_all_results("classification_results")
