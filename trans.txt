from sklearn.metrics import mean_squared_error, mean_absolute_error
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK

def tune_hyperparams(data, target_col, eval_data, max_evals=30, metric="rmse", random_state=42):
    """
    Подбор гиперпараметров LGBM через Hyperopt (TPE). Обучает на data, минимизирует метрику на eval_data.
    Возвращает (best_model, best_params). data и eval_data — датафреймы с признаками и target_col.
    """
    X = data.drop(columns=[target_col])
    y = data[target_col]
    X_ev = eval_data.drop(columns=[target_col])
    y_ev = eval_data[target_col].values
    fn = mean_squared_error if metric == "rmse" else mean_absolute_error

    space = {
        "n_estimators": hp.quniform("n_estimators", 50, 300, 25),
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "learning_rate": hp.loguniform("learning_rate", np.log(0.02), np.log(0.3)),
        "num_leaves": hp.quniform("num_leaves", 15, 127, 10),
        "min_child_samples": hp.quniform("min_child_samples", 5, 50, 5),
    }

    def objective(params):
        p = {
            "n_estimators": int(params["n_estimators"]),
            "max_depth": int(params["max_depth"]),
            "learning_rate": params["learning_rate"],
            "num_leaves": int(params["num_leaves"]),
            "min_child_samples": int(params["min_child_samples"]),
        }
        m = lgb.LGBMRegressor(random_state=random_state, verbosity=-1, **p)
        m.fit(X, y, eval_set=[(X_ev, y_ev)])
        pred = m.predict(X_ev)
        score = np.sqrt(fn(y_ev, pred)) if metric == "rmse" else fn(y_ev, pred)
        return {"loss": score, "status": STATUS_OK}

    trials = Trials()
    best = fmin(
        objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials,
        rstate=np.random.default_rng(random_state),
    )
    best_params = {
        "n_estimators": int(best["n_estimators"]),
        "max_depth": int(best["max_depth"]),
        "learning_rate": best["learning_rate"],
        "num_leaves": int(best["num_leaves"]),
        "min_child_samples": int(best["min_child_samples"]),
    }
    best_model = lgb.LGBMRegressor(random_state=random_state, verbosity=-1, **best_params)
    best_model.fit(X, y, eval_set=[(X_ev, y_ev)])
    return best_model, best_params
