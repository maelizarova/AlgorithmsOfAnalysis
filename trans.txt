import pandas as pd
from typing import Dict, List, Optional, Union
from sklearn.base import BaseEstimator
from sklearn.preprocessing import OrdinalEncoder

def select_top_features_with_eval_set(
    data: pd.DataFrame,
    targets: List[str],
    model: BaseEstimator,
    eval_set: Optional[pd.DataFrame] = None,
    n_top: int = 10,
    categorical_features: Optional[List[str]] = None,
    preprocess_categorical: bool = True,
    early_stopping_rounds: Optional[int] = None,
    random_state: int = 42,
    fit_params: Optional[dict] = None
) -> Dict[str, List[str]]:
    """
    Универсальная функция для отбора фичей с явным словарем fit_params.
    
    Параметры:
    ----------
    fit_params : dict, optional
        Словарь параметров для метода fit() модели.
        Пример: {'eval_metric': 'rmse', 'verbose': True}
    """
    top_features = {}
    
    # Инициализация словаря параметров
    if fit_params is None:
        fit_params = {}
    
    for target in targets:
        # Подготовка данных
        X = data.drop(columns=targets)
        y = data[target]
        
        # Определение категориальных фичей
        if categorical_features is None:
            categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # Кодирование категорий
        if preprocess_categorical and categorical_features:
            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
            X[categorical_features] = encoder.fit_transform(X[categorical_features])
            
        # Подготовка eval_set
        current_fit_params = fit_params.copy()
        
        if eval_set is not None:
            X_valid = eval_set.drop(columns=targets)
            y_valid = eval_set[target]
            
            if preprocess_categorical and categorical_features:
                X_valid[categorical_features] = encoder.transform(X_valid[categorical_features])
            
            current_fit_params['eval_set'] = [(X_valid, y_valid)]
            
            if early_stopping_rounds is not None:
                current_fit_params['early_stopping_rounds'] = early_stopping_rounds
        
        # Обучение модели
        model.set_params(random_state=random_state)
        model.fit(X, y, **current_fit_params)
        
        # Получение важности фичей
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
        else:
            from sklearn.inspection import permutation_importance
            metric = current_fit_params.get('eval_metric', 'rmse')
            scorer = get_scorer(metric) if isinstance(metric, str) else metric
            result = permutation_importance(
                model, X, y, scoring=scorer, n_repeats=5, random_state=random_state
            )
            importance = result.importances_mean
        
        # Сохранение результатов
        top_features[target] = pd.DataFrame({
            'feature': X.columns,
            'importance': importance
        }).sort_values('importance', ascending=False).head(n_top)['feature'].tolist()
    
    return top_features

from lightgbm import LGBMRegressor

model = LGBMRegressor(n_estimators=1000)
fit_params = {
    'eval_metric': 'rmse',
    'verbose': 10,
    'callbacks': [early_stopping(50)]
}

selected_features = select_top_features_with_eval_set(
    data=train_data,
    targets=['sales'],
    model=model,
    eval_set=valid_data,
    categorical_features=['category'],
    preprocess_categorical=False,
    early_stopping_rounds=50,
    fit_params=fit_params
)
