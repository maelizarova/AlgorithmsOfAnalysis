def objective(params):
    params = {
        'n_estimators': int(params['n_estimators']),
        'max_depth': int(params['max_depth']),
        'learning_rate': params['learning_rate'],
        'num_leaves': int(params['num_leaves']),
        'min_child_samples': int(params['min_child_samples'])
    }

    model = lgb.LGBMClassifier(
            n_estimators = params['n_estimators'],
            max_depth = params['max_depth'],
            learning_rate = params['learning_rate'],
            num_leaves = params['num_leaves'],
            min_child_samples = params['min_child_samples'],
            random_state = SEED,
            n_jobs=-1
    )

    model.fit(train[best_features], train[TARGET])

    y_pred = model.predict_proba(test[best_features])[:, 1]

    roc_auc = roc_auc_score(test[TARGET], y_pred)

    # оптимизация Hyperopt минимизирует loss, поэтому берём -roc_auc
    return {'loss': -roc_auc, 'status': STATUS_OK}

space = {
    'n_estimators': hp.quniform('n_estimators', 100, 300, 1),
    'max_depth': hp.quniform('max_depth', 3, 10, 1),
    'learning_rate': hp.loguniform('learning_rate', -4, -1),
    'num_leaves': hp.quniform('num_leaves', 20, 60, 1),
    'min_child_samples': hp.quniform('min_child_samples', 5, 30, 1)
}

trials = Trials()

best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,  # количество итераций - можно менять, в зависимости от размеров трейна; можно уменьшить, если кернел умирает
    trials=trials
)
