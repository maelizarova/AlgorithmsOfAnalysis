import pandas as pd
import json
from joblib import Parallel, delayed
import time
from datetime import datetime

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π –∂–∞–ª–æ–±—ã
def process_single_complaint(complaint_data, products_kb, categories_kb, index):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∂–∞–ª–æ–±—É: –æ—á–∏—Å—Ç–∫–∞, —ç–º–±–µ–¥–¥–∏–Ω–≥, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    """
    try:
        complaint_text = complaint_data['text']  # –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ –∫–æ–ª–æ–Ω–∫–µ 'text'
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = clean_complaint_text(complaint_text)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        embedding = get_embedding(cleaned_text)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        individual_prompt = create_prompt(products_kb, categories_kb, cleaned_text)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification_result = call_llm_api(individual_prompt)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            if hasattr(classification_result, 'content'):
                classification_data = json.loads(classification_result.content)
            else:
                classification_data = json.loads(classification_result)
        except:
            classification_data = {
                "product": "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù",
                "complaint_type": "–î–†–£–ì–û–ï", 
                "complaint_subtype": "–î–†–£–ì–û–ï",
                "confidence": "–Ω–∏–∑–∫–∞—è",
                "explanation": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞"
            }
        
        result = {
            'id': index + 1,
            'original_text': complaint_text,
            'cleaned_text': cleaned_text,
            'embedding': embedding,
            'embedding_dimension': len(embedding) if embedding else 0,
            'prompt': individual_prompt,
            'classification': classification_data,
            'processed_at': datetime.now().isoformat(),
            'batch_index': index
        }
        
        return result
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∂–∞–ª–æ–±—ã {index}: {e}")
        return {
            'id': index + 1,
            'original_text': complaint_data.get('text', ''),
            'cleaned_text': '',
            'embedding': None,
            'embedding_dimension': 0,
            'prompt': '',
            'classification': {
                "product": "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù",
                "complaint_type": "–î–†–£–ì–û–ï", 
                "complaint_subtype": "–î–†–£–ì–û–ï",
                "confidence": "–Ω–∏–∑–∫–∞—è",
                "explanation": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            },
            'processed_at': datetime.now().isoformat(),
            'batch_index': index
        }

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
def process_complaints_parallel(complaints_df, batch_size=100, n_jobs=-1, output_dir="classification_results"):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∂–∞–ª–æ–± —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π
    products_kb, categories_kb = load_knowledge_bases()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    complaints_list = complaints_df.to_dict('records')
    total_complaints = len(complaints_list)
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {total_complaints} –∂–∞–ª–æ–±")
    print(f"üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä: {n_jobs if n_jobs != -1 else '–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ'}")
    print(f"üì¶ –†–∞–∑–º–µ—Ä batch: {batch_size}")
    print("‚îÄ" * 50)
    
    start_time = time.time()
    all_results = []
    batch_number = 1
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    for batch_start in range(0, total_complaints, batch_size):
        batch_end = min(batch_start + batch_size, total_complaints)
        batch_complaints = complaints_list[batch_start:batch_end]
        batch_indices = list(range(batch_start, batch_end))
        
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ batch {batch_number}: {batch_start}-{batch_end-1}")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_results = Parallel(n_jobs=n_jobs, verbose=10)(
            delayed(process_single_complaint)(
                complaint, products_kb, categories_kb, idx
            ) for complaint, idx in zip(batch_complaints, batch_indices)
        )
        
        all_results.extend(batch_results)
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
        save_results(batch_results, batch_number, batch_end, output_dir)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        elapsed_time = time.time() - start_time
        estimated_total = (elapsed_time / batch_end) * total_complaints
        remaining_time = estimated_total - elapsed_time
        
        print(f"‚úÖ Batch {batch_number} –∑–∞–≤–µ—Ä—à–µ–Ω: {batch_end}/{total_complaints}")
        print(f"‚è±Ô∏è  –ü—Ä–æ—à–ª–æ: {elapsed_time/60:.1f} –º–∏–Ω | –û—Å—Ç–∞–ª–æ—Å—å: ~{remaining_time/60:.1f} –º–∏–Ω")
        print("‚îÄ" * 50)
        
        batch_number += 1
    
    total_time = time.time() - start_time
    print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_complaints/(total_time/60):.1f} –∂–∞–ª–æ–±/–º–∏–Ω")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    merge_all_results(output_dir)
    
    return all_results

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –ª–∏–º–∏—Ç—ã API)
def process_complaints_with_rate_limit(complaints_df, batch_size=100, n_jobs=2, requests_per_minute=60, output_dir="classification_results"):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    import time
    from joblib import Parallel, delayed
    
    products_kb, categories_kb = load_knowledge_bases()
    complaints_list = complaints_df.to_dict('records')
    total_complaints = len(complaints_list)
    
    print(f"üöÄ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏")
    print(f"üîß {n_jobs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞")
    print(f"üìä –õ–∏–º–∏—Ç: {requests_per_minute} –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω")
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
    delay_between_batches = (60 / requests_per_minute) * (batch_size / n_jobs)
    
    all_results = []
    batch_number = 1
    
    for batch_start in range(0, total_complaints, batch_size):
        batch_end = min(batch_start + batch_size, total_complaints)
        batch_complaints = complaints_list[batch_start:batch_end]
        batch_indices = list(range(batch_start, batch_end))
        
        print(f"üîÑ Batch {batch_number}: {batch_start}-{batch_end-1}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_results = Parallel(n_jobs=n_jobs)(
            delayed(process_single_complaint)(
                complaint, products_kb, categories_kb, idx
            ) for complaint, idx in zip(batch_complaints, batch_indices)
        )
        
        all_results.extend(batch_results)
        save_results(batch_results, batch_number, batch_end, output_dir)
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ API
        if batch_end < total_complaints:
            print(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay_between_batches:.1f} —Å–µ–∫...")
            time.sleep(delay_between_batches)
        
        batch_number += 1
    
    merge_all_results(output_dir)
    return all_results

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # complaints_df = pd.read_csv('your_complaints.csv')
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ —è–¥—Ä–∞)
    print("=== –í–ê–†–ò–ê–ù–¢ 1: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –°–ö–û–†–û–°–¢–¨ ===")
    results1 = process_complaints_parallel(
        complaints_df=complaints_df,
        batch_size=50,           # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        n_jobs=-1,               # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞
        output_dir="parallel_results"
    )
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –° –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –ª–∏–º–∏—Ç—ã API)
    print("\n=== –í–ê–†–ò–ê–ù–¢ 2: –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú –°–ö–û–†–û–°–¢–ò ===")
    results2 = process_complaints_with_rate_limit(
        complaints_df=complaints_df,
        batch_size=30,           # –ú–µ–Ω—å—à–∏–π –±–∞—Ç—á
        n_jobs=2,                # –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        requests_per_minute=30,  # –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
        output_dir="rate_limited_results"
    )
