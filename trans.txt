import re
import json
import os
import numpy as np
import pandas as pd
from datetime import datetime
from joblib import Parallel, delayed
import time

# ========== –ë–ê–ó–û–í–´–ï –§–£–ù–ö–¶–ò–ò ==========

def clean_complaint_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —á–∏—Å–µ–ª"""
    cleaned_text = re.sub(r'\d+', '', text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

def get_embedding(text, model="embedding-model"):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à API –≤—ã–∑–æ–≤)"""
    try:
        # –ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –í–ê–® –†–ï–ê–õ–¨–ù–´–ô –í–´–ó–û–í API
        # response = call_gpt_oss_embedding(text)
        # embedding = response.embedding
        embedding = np.random.randn(512).tolist()
        return embedding
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return None

def load_knowledge_bases():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π"""
    with open('products_knowledge_base.json', 'r', encoding='utf-8') as f:
        products_kb = json.load(f)
    with open('complaint_categories_knowledge_base.json', 'r', encoding='utf-8') as f:
        complaint_categories_kb = json.load(f)
    return products_kb, complaint_categories_kb

def create_prompt(products_kb, categories_kb, complaint_text):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    available_products = [product['name'] for product in products_kb['products']]
    available_types = list(set([category['type'] for category in categories_kb['complaint_categories']]))
    
    prompt_template = """
# –†–æ–ª—å
–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∞–ª–æ–±. –°–æ–ø–æ—Å—Ç–∞–≤—å –∂–∞–ª–æ–±—É —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.

# –°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –≥–æ—Ç–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
- –ù–ï –∏–∑–º–µ–Ω—è–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

# –î–û–°–¢–£–ü–ù–´–ï –ü–†–û–î–£–ö–¢–´ (–≤—ã–±–µ—Ä–∏ –û–î–ò–ù):
{available_products}

# –î–û–°–¢–£–ü–ù–´–ï –¢–ò–ü–´ –ñ–ê–õ–û–ë (–≤—ã–±–µ—Ä–∏ –û–î–ò–ù):
{available_types}

# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
  "classification": {{
    "type": "–≤—ã–±—Ä–∞–Ω–Ω—ã–π_—Ç–∏–ø_–∏–∑_—Å–ø–∏—Å–∫–∞",
    "product": "–≤—ã–±—Ä–∞–Ω–Ω—ã–π_–ø—Ä–æ–¥—É–∫—Ç_–∏–∑_—Å–ø–∏—Å–∫–∞", 
    "confidence_level": "–≤—ã—Å–æ–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–Ω–∏–∑–∫–∏–π",
    "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"
  }}
}}

–ñ–∞–ª–æ–±–∞: "{complaint_text}"

–û—Ç–≤–µ—Ç:
"""
    
    return prompt_template.format(
        available_products=", ".join(available_products),
        available_types=", ".join(available_types),
        complaint_text=complaint_text
    )

# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–°–ü–ê–†–ê–õ–õ–ï–õ–ò–í–ê–ù–ò–Ø ==========

def process_single_complaint(complaint_data, products_kb, categories_kb, index, text_column='text'):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∂–∞–ª–æ–±—ã"""
    try:
        complaint_text = complaint_data[text_column]
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = clean_complaint_text(complaint_text)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        embedding = get_embedding(cleaned_text)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        individual_prompt = create_prompt(products_kb, categories_kb, cleaned_text)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –í–ê–® API –í–´–ó–û–í)
        classification_result = call_llm_api(individual_prompt)  # –í–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –Ω–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
        try:
            if hasattr(classification_result, 'content'):
                response_data = json.loads(classification_result.content)
            else:
                response_data = json.loads(classification_result)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            classification_data = response_data.get('classification', {})
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –∂–∞–ª–æ–±—ã {index}: {e}")
            classification_data = {
                "type": "–î–†–£–ì–û–ï",
                "product": "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù", 
                "confidence_level": "–Ω–∏–∑–∫–∏–π",
                "explanation": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}"
            }
        
        result = {
            'id': index + 1,
            'original_text': complaint_text,
            'cleaned_text': cleaned_text,
            'embedding': embedding,
            'embedding_dimension': len(embedding) if embedding else 0,
            'prompt': individual_prompt,
            'classification': classification_data,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –Ω–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏
            'processed_at': datetime.now().isoformat(),
            'batch_index': index
        }
        
        return result
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∂–∞–ª–æ–±—ã {index}: {e}")
        return {
            'id': index + 1,
            'original_text': complaint_data.get(text_column, ''),
            'cleaned_text': '',
            'embedding': None,
            'embedding_dimension': 0,
            'prompt': '',
            'classification': {
                "type": "–î–†–£–ì–û–ï",
                "product": "–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù", 
                "confidence_level": "–Ω–∏–∑–∫–∏–π",
                "explanation": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            },
            'processed_at': datetime.now().isoformat(),
            'batch_index': index
        }

# ========== –§–£–ù–ö–¶–ò–ò –°–û–•–†–ê–ù–ï–ù–ò–Ø ==========

def save_results(results, batch_number, total_processed, output_dir="classification_results"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{output_dir}/batch_{batch_number}_{timestamp}.json"
    
    results_data = {
        "metadata": {
            "batch_number": batch_number,
            "total_processed": total_processed,
            "timestamp": timestamp,
            "saved_at": datetime.now().isoformat(),
            "embedding_dimension": len(results[0]['embedding']) if results and 'embedding' in results[0] else 0
        },
        "results": results
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"‚úì Batch {batch_number} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({total_processed} –∂–∞–ª–æ–±)")
    return filename

def merge_all_results(output_dir="classification_results"):
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö batch'–µ–π"""
    if not os.path.exists(output_dir):
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    all_results = []
    batch_files = [f for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.json')]
    
    for batch_file in sorted(batch_files):
        batch_path = os.path.join(output_dir, batch_file)
        with open(batch_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_results.extend(data['results'])
    
    merged_file = f"{output_dir}/all_results_merged.json"
    with open(merged_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã: {merged_file}")
    print(f"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_results)}")
    
    return all_results

# ========== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–ü–ê–†–ê–õ–õ–ï–õ–ò–í–ê–ù–ò–Ø ==========

def process_complaints_parallel(complaints_df, batch_size=100, n_jobs=-1, output_dir="classification_results", text_column='text'):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∂–∞–ª–æ–±
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π
    products_kb, categories_kb = load_knowledge_bases()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    complaints_list = complaints_df.to_dict('records')
    total_complaints = len(complaints_list)
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {total_complaints} –∂–∞–ª–æ–±")
    print(f"üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä: {n_jobs if n_jobs != -1 else '–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ'}")
    print(f"üì¶ –†–∞–∑–º–µ—Ä batch: {batch_size}")
    print("‚îÄ" * 50)
    
    start_time = time.time()
    all_results = []
    batch_number = 1
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
    for batch_start in range(0, total_complaints, batch_size):
        batch_end = min(batch_start + batch_size, total_complaints)
        batch_complaints = complaints_list[batch_start:batch_end]
        batch_indices = list(range(batch_start, batch_end))
        
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ batch {batch_number}: {batch_start}-{batch_end-1}")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_results = Parallel(n_jobs=n_jobs, verbose=10)(
            delayed(process_single_complaint)(
                complaint, products_kb, categories_kb, idx, text_column
            ) for complaint, idx in zip(batch_complaints, batch_indices)
        )
        
        all_results.extend(batch_results)
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_results(batch_results, batch_number, batch_end, output_dir)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        elapsed_time = time.time() - start_time
        estimated_total = (elapsed_time / batch_end) * total_complaints
        remaining_time = estimated_total - elapsed_time
        
        print(f"‚úÖ Batch {batch_number} –∑–∞–≤–µ—Ä—à–µ–Ω: {batch_end}/{total_complaints}")
        print(f"‚è±Ô∏è  –ü—Ä–æ—à–ª–æ: {elapsed_time/60:.1f} –º–∏–Ω | –û—Å—Ç–∞–ª–æ—Å—å: ~{remaining_time/60:.1f} –º–∏–Ω")
        print("‚îÄ" * 50)
        
        batch_number += 1
    
    total_time = time.time() - start_time
    print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_complaints/(total_time/60):.1f} –∂–∞–ª–æ–±/–º–∏–Ω")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    merge_all_results(output_dir)
    
    return all_results

# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø –ò–¢–û–ì–û–í–û–ì–û –î–ê–¢–ê–§–†–ï–ô–ú–ê ==========

def create_comparison_dataframe(merged_results_file, original_df):
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º"""
    
    with open(merged_results_file, 'r', encoding='utf-8') as f:
        classification_results = json.load(f)
    
    model_data = []
    
    for result in classification_results:
        classification = result.get('classification', {})
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã classification
        if isinstance(classification, str):
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                classification = json.loads(classification)
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º classification
                if 'classification' in classification:
                    classification = classification['classification']
            except:
                classification = {}
        elif not isinstance(classification, dict):
            classification = {}
        
        model_data.append({
            'index': result['id'] - 1,
            '–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç': classification.get('product', '–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù'),
            '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã': classification.get('type', '–î–†–£–ì–û–ï'),  # —Ç–µ–ø–µ—Ä—å 'type' –≤–º–µ—Å—Ç–æ 'complaint_type'
            '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å': classification.get('confidence_level', '–Ω–∏–∑–∫–∏–π'),  # —Ç–µ–ø–µ—Ä—å 'confidence_level'
            '–º–æ–¥–µ–ª—å_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ': classification.get('explanation', ''),
            '–∏—Å—Ö–æ–¥–Ω—ã–π_—Ç–µ–∫—Å—Ç_–∂–∞–ª–æ–±—ã': result.get('original_text', '')
        })
    
    model_df = pd.DataFrame(model_data)
    model_df = model_df.set_index('index')
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_df = original_df.copy()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–∏
    for col in ['–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç', '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã', '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–º–æ–¥–µ–ª—å_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ', '–∏—Å—Ö–æ–¥–Ω—ã–π_—Ç–µ–∫—Å—Ç_–∂–∞–ª–æ–±—ã_–º–æ–¥–µ–ª—å']:
        comparison_df[col] = model_df[col]
    
    return comparison_df

# ========== –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ==========

if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏)
    # complaints_df = pd.read_csv('your_complaints.csv')
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = process_complaints_parallel(
        complaints_df=complaints_df,
        batch_size=100,
        n_jobs=-1,
        output_dir="parallel_results",
        text_column='text'
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    comparison_df = create_comparison_dataframe(
        merged_results_file='parallel_results/all_results_merged.json',
        original_df=complaints_df
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    comparison_df.to_csv('final_classified_complaints.csv', index=False, encoding='utf-8-sig')
    print("‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ final_classified_complaints.csv")
    
    # –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n=== –ü–ï–†–í–´–ï 5 –ó–ê–ü–ò–°–ï–ô ===")
    print(comparison_df[['–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç', '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã', '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å']].head())
