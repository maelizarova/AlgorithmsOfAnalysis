import pandas as pd
import numpy as np
from typing import Dict, List, Union, Any, Tuple
from sklearn.base import BaseEstimator
from sklearn.metrics import get_scorer
from sklearn.model_selection import train_test_split

def select_top_features(
    data: pd.DataFrame,
    targets: List[str],
    model: BaseEstimator,
    n_top: int = 10,
    categorical_features: List[str] = None,
    preprocess_categorical: bool = True,
    metric: Union[str, callable] = None,
    greater_is_better: bool = None,
    eval_set_size: float = 0.2,
    early_stopping_rounds: int = None,
    random_state: int = 42,
    verbose: bool = False,
    **fit_params: Any
) -> Dict[str, List[str]]:
    """
    Отбирает топ-N фичей для каждого целевого признака с поддержкой eval_set и ранней остановки.

    Параметры:
    ----------
    eval_set_size : float, optional (default=0.2)
        Размер валидационного набора (если 0, eval_set не используется).
    early_stopping_rounds : int, optional
        Количество раундов без улучшения для остановки (требуется eval_set).
    verbose : bool, optional (default=False)
        Вывод логов обучения.
    """
    top_features = {}
    
    for target in targets:
        X = data.drop(columns=targets)
        y = data[target]
        
        # Обработка категориальных фичей
        if categorical_features is None:
            categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if preprocess_categorical and categorical_features:
            X_cat = X[categorical_features].copy()
            if not hasattr(model, 'set_categorical_feature'):
                from sklearn.preprocessing import OrdinalEncoder
                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                X[categorical_features] = encoder.fit_transform(X_cat)
        
        # Разделение на train/valid (если eval_set_size > 0)
        eval_set = None
        if eval_set_size > 0:
            X_train, X_valid, y_train, y_valid = train_test_split(
                X, y, test_size=eval_set_size, random_state=random_state
            )
            eval_set = [(X_valid, y_valid)]
            
            # Для CatBoost/LightGBM в fit_params
            if hasattr(model, 'fit'):
                fit_params.setdefault('eval_set', eval_set)
                if early_stopping_rounds:
                    fit_params['early_stopping_rounds'] = early_stopping_rounds
        else:
            X_train, y_train = X, y
        
        # Обучение модели
        if hasattr(model, 'set_categorical_feature') and categorical_features:
            model.set_categorical_feature(categorical_features)
        
        model.set_params(random_state=random_state)
        
        if verbose:
            print(f"Обучение модели для target: {target}")
        
        if hasattr(model, 'fit'):
            model.fit(X_train, y_train, **fit_params)
        else:
            raise ValueError("Модель должна иметь метод fit!")
        
        # Получение важности фичей
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
        elif metric is not None:
            from sklearn.inspection import permutation_importance
            scorer = get_scorer(metric) if isinstance(metric, str) else metric
            result = permutation_importance(
                model, X_train, y_train, scoring=scorer, n_repeats=5, random_state=random_state
            )
            importance = result.importances_mean
        else:
            raise ValueError(
                "У модели нет feature_importances_ и не указана метрика. "
                "Используйте permutation_importance, передав metric."
            )
        
        # Сортировка фичей
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': importance
        }).sort_values('importance', ascending=not greater_is_better if greater_is_better else True)
        
        top_features[target] = feature_importance['feature'].head(n_top).tolist()
    
    return top_features

from lightgbm import LGBMRegressor

model = LGBMRegressor(n_estimators=1000)
targets = ['sales', 'revenue']
categoricals = ['product_category', 'region']

selected_features = select_top_features(
    data=df,
    targets=targets,
    model=model,
    categorical_features=categoricals,
    eval_set_size=0.2,
    early_stopping_rounds=50,
    verbose=True,
    fit_params={
        'eval_metric': 'rmse',  # Метрика для валидации
        'callbacks': [log_evaluation(10)]  # Вывод логов каждые 10 итераций
    }
)
