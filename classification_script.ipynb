{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import requests\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "\n",
        "DEFAULT_CLASSIFICATION = {\n",
        "    \"type\": \"–î–†–£–ì–û–ï\",\n",
        "    \"product\": \"–ù–ï–û–ü–†–ï–î–ï–õ–ï–ù\",\n",
        "    \"confidence_level\": \"–Ω–∏–∑–∫–∏–π\",\n",
        "    \"explanation\": \"\"\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_gpt_oss(prompt, model=\"gpt-oss:120b\"):\n",
        "    url = ''\n",
        "    api_key = ''\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, stream=True, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        logging.info(f\"response code - {response.status_code}\")\n",
        "        full_response = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                json_response = json.loads(line)\n",
        "                token = json_response.get(\"response\", \"\")\n",
        "                full_response += token\n",
        "                print(token, end=\"\", flush=True)\n",
        "        print()\n",
        "        return full_response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling Ollama API: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def call_gpt_oss_embedding(prompt, model=\"bge-m3:latest\"):\n",
        "    url = ''\n",
        "    api_key = ''\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "    response = requests.post(url, json=payload, stream=False, headers=headers)\n",
        "    embedding = json.loads(response.content.decode('utf-8'))['embedding']\n",
        "    return embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_complaint_text(text):\n",
        "    cleaned_text = re.sub(r'\\d+', '', text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def load_knowledge_bases():\n",
        "    with open('products.json', 'r', encoding='utf-8') as f:\n",
        "        products_kb = json.load(f)\n",
        "    with open('types_products.json', 'r', encoding='utf-8') as f:\n",
        "        complaint_categories_kb = json.load(f)\n",
        "    return products_kb, complaint_categories_kb\n",
        "\n",
        "\n",
        "def create_prompt(products_kb, categories_kb, complaint_text):\n",
        "    prompt_template = \"\"\"\n",
        "# –†–æ–ª—å\n",
        "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –±–∞–Ω–∫–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –∂–∞–ª–æ–±—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤.\n",
        "\n",
        "# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π\n",
        "\n",
        "## –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:\n",
        "{products_json}\n",
        "\n",
        "## –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –∂–∞–ª–æ–± –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:\n",
        "{types_products_json}\n",
        "\n",
        "# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
        "1. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∂–∞–ª–æ–±—ã: –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–π –∂–∞–ª–æ–±—É –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Ñ—Ä–∞–∑—ã –∏ —Å—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã.\n",
        "2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ü—Ä–æ–¥—É–∫—Ç–∞: –°–æ–ø–æ—Å—Ç–∞–≤—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∂–∞–ª–æ–±—ã —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–∑ products.json. –í—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–¥—É–∫—Ç –∏–∑ —Å–ø–∏—Å–∫–∞. –ï—Å–ª–∏ –∂–∞–ª–æ–±–∞ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –≤—ã–±–µ—Ä–∏ –æ—Å–Ω–æ–≤–Ω–æ–π.\n",
        "3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¢–∏–ø–∞: –ò—Å–ø–æ–ª—å–∑—É–π types_products.json –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ç–∏–ø–∞. –£—á–∏—Ç—ã–≤–∞–π —Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–∏–ø–æ–º –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–º - —Ç–∏–ø –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–æ–º. –°–æ–ø–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∂–∞–ª–æ–±—ã —Å type_tags –∏ type_description.\n",
        "\n",
        "# –°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤\n",
        "- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "- –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞—Ö\n",
        "- –ù–∞–∑–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º\n",
        "\n",
        "# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):\n",
        "{{\n",
        "    \"type\": \"–Ω–∞–∑–≤–∞–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞\",\n",
        "    \"product\": \"–Ω–∞–∑–≤–∞–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞\",\n",
        "    \"confidence_level\": \"–≤—ã—Å–æ–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–Ω–∏–∑–∫–∏–π\",\n",
        "    \"explanation\": \"–∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞\"\n",
        "}}\n",
        "\n",
        "–¢–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: \n",
        "{complaint_text}\n",
        "\n",
        "–¢–≤–æ–π –æ—Ç–≤–µ—Ç:\n",
        "\"\"\"\n",
        "    return prompt_template.format(\n",
        "        products_json=json.dumps(products_kb, ensure_ascii=False, indent=2),\n",
        "        types_products_json=json.dumps(categories_kb, ensure_ascii=False, indent=2),\n",
        "        complaint_text=complaint_text\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_classification_response(response_text, error_message=\"\"):\n",
        "    if not response_text:\n",
        "        classification = DEFAULT_CLASSIFICATION.copy()\n",
        "        classification[\"explanation\"] = f\"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏. {error_message}\".strip()\n",
        "        return classification\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response_text)\n",
        "        if isinstance(parsed, dict) and isinstance(parsed.get(\"classification\"), dict):\n",
        "            parsed = parsed[\"classification\"]\n",
        "\n",
        "        return {\n",
        "            \"type\": parsed.get(\"type\", DEFAULT_CLASSIFICATION[\"type\"]),\n",
        "            \"product\": parsed.get(\"product\", DEFAULT_CLASSIFICATION[\"product\"]),\n",
        "            \"confidence_level\": parsed.get(\"confidence_level\", DEFAULT_CLASSIFICATION[\"confidence_level\"]),\n",
        "            \"explanation\": parsed.get(\"explanation\", DEFAULT_CLASSIFICATION[\"explanation\"])\n",
        "        }\n",
        "    except (json.JSONDecodeError, AttributeError, TypeError) as exc:\n",
        "        classification = DEFAULT_CLASSIFICATION.copy()\n",
        "        classification[\"explanation\"] = f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {exc}. {error_message}\".strip()\n",
        "        return classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_complaint(complaint_text, products_kb, categories_kb, index, text_column='text'):\n",
        "    try:\n",
        "        cleaned_text = clean_complaint_text(complaint_text)\n",
        "        embedding = call_gpt_oss_embedding(cleaned_text, model=\"bge-m3:latest\")\n",
        "        print(embedding)\n",
        "\n",
        "        individual_prompt = create_prompt(products_kb, categories_kb, cleaned_text)\n",
        "        classification_response = call_gpt_oss(individual_prompt, model=\"gpt-oss:120b\")\n",
        "        classification_data = parse_classification_response(classification_response, error_message=f\"–ñ–∞–ª–æ–±–∞ #{index + 1}\")\n",
        "\n",
        "        return {\n",
        "            'id': index + 1,\n",
        "            'original_text': complaint_text,\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'embedding': embedding,\n",
        "            'embedding_dimension': len(embedding) if embedding else 0,\n",
        "            'prompt': individual_prompt,\n",
        "            'classification': classification_data,\n",
        "            'processed_at': datetime.now().isoformat(),\n",
        "            'batch_index': index\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∂–∞–ª–æ–±—ã {index}: {e}\")\n",
        "        fallback = DEFAULT_CLASSIFICATION.copy()\n",
        "        fallback[\"explanation\"] = f\"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}\"\n",
        "        return {\n",
        "            'id': index + 1,\n",
        "            'original_text': complaint_text,\n",
        "            'cleaned_text': '',\n",
        "            'embedding': None,\n",
        "            'embedding_dimension': 0,\n",
        "            'prompt': '',\n",
        "            'classification': fallback,\n",
        "            'processed_at': datetime.now().isoformat(),\n",
        "            'batch_index': index\n",
        "        }\n",
        "\n",
        "\n",
        "def save_results(results, batch_number, total_processed, output_dir=\"classification_results\"):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{output_dir}/batch_{batch_number}_{timestamp}.json\"\n",
        "    print(results)\n",
        "\n",
        "    results_data = {\n",
        "        \"metadata\": {\n",
        "            \"batch_number\": batch_number,\n",
        "            \"total_processed\": total_processed,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"saved_at\": datetime.now().isoformat(),\n",
        "            \"embedding_dimension\": len(results[0]['embedding']) if results and results[0].get('embedding') else 0\n",
        "        },\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results_data, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    print(f\"‚úì Batch {batch_number} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} ({total_processed} –∂–∞–ª–æ–±)\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "def merge_all_results(output_dir=\"classification_results\"):\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(\"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")\n",
        "        return\n",
        "\n",
        "    all_results = []\n",
        "    batch_files = [f for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.json')]\n",
        "\n",
        "    for batch_file in sorted(batch_files):\n",
        "        batch_path = os.path.join(output_dir, batch_file)\n",
        "        with open(batch_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            all_results.extend(data['results'])\n",
        "\n",
        "    merged_file = f\"{output_dir}/all_results_merged.json\"\n",
        "    with open(merged_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    print(f\"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã: {merged_file}\")\n",
        "    print(f\"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_results)}\")\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def process_complaints_parallel(complaints, batch_size=100, n_jobs=-1, output_dir=\"classification_results\", text_column='text'):\n",
        "    products_kb, categories_kb = load_knowledge_bases()\n",
        "    total_complaints = len(complaints)\n",
        "\n",
        "    print(f\"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {total_complaints} –∂–∞–ª–æ–±\")\n",
        "    print(f\"üîß –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä: {n_jobs if n_jobs != -1 else '–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ'}\")\n",
        "    print(f\"üì¶ –†–∞–∑–º–µ—Ä batch: {batch_size}\")\n",
        "    print(\"‚îÄ\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "    all_results = []\n",
        "    batch_number = 1\n",
        "\n",
        "    for batch_start in range(0, total_complaints, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, total_complaints)\n",
        "        batch_complaints = complaints[batch_start:batch_end]\n",
        "        batch_indices = list(range(batch_start, batch_end))\n",
        "\n",
        "        print(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ batch {batch_number}: {batch_start}-{batch_end-1}\")\n",
        "        batch_results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
        "            delayed(process_single_complaint)(\n",
        "                complaint, products_kb, categories_kb, idx\n",
        "            ) for idx, complaint in zip(batch_indices, batch_complaints)\n",
        "        )\n",
        "\n",
        "        all_results.extend(batch_results)\n",
        "        save_results(batch_results, batch_number, batch_end, output_dir)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        estimated_total = (elapsed_time / batch_end) * total_complaints if batch_end else 0\n",
        "        remaining_time = estimated_total - elapsed_time\n",
        "\n",
        "        print(f\"‚úÖ Batch {batch_number} –∑–∞–≤–µ—Ä—à–µ–Ω: {batch_end}/{total_complaints}\")\n",
        "        print(f\"‚è±Ô∏è  –ü—Ä–æ—à–ª–æ: {elapsed_time/60:.1f} –º–∏–Ω | –û—Å—Ç–∞–ª–æ—Å—å: ~{remaining_time/60:.1f} –º–∏–Ω\")\n",
        "        print(\"‚îÄ\" * 50)\n",
        "\n",
        "        batch_number += 1\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç\")\n",
        "    print(f\"üìä –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_complaints/(total_time/60):.1f} –∂–∞–ª–æ–±/–º–∏–Ω\")\n",
        "\n",
        "    merge_all_results(output_dir)\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def create_comparison_dataframe(merged_results_file, original_df):\n",
        "    with open(merged_results_file, 'r', encoding='utf-8') as f:\n",
        "        classification_results = json.load(f)\n",
        "\n",
        "    model_data = []\n",
        "    for result in classification_results:\n",
        "        classification = result.get('classification', DEFAULT_CLASSIFICATION)\n",
        "        if isinstance(classification, str):\n",
        "            classification = parse_classification_response(classification)\n",
        "        elif not isinstance(classification, dict):\n",
        "            classification = DEFAULT_CLASSIFICATION.copy()\n",
        "\n",
        "        model_data.append({\n",
        "            'index': result['id'] - 1,\n",
        "            '–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç': classification.get('product', '---'),\n",
        "            '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã': classification.get('type', '---'),\n",
        "            '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å': classification.get('confidence_level', '–Ω–∏–∑–∫–∏–π'),\n",
        "            '–º–æ–¥–µ–ª—å_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ': classification.get('explanation', '')\n",
        "        })\n",
        "\n",
        "    model_df = pd.DataFrame(model_data)\n",
        "    model_df = model_df.set_index('index')\n",
        "\n",
        "    comparison_df = original_df.copy()\n",
        "    for col in ['–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç', '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã', '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–º–æ–¥–µ–ª—å_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ']:\n",
        "        comparison_df[col] = model_df[col]\n",
        "\n",
        "    return comparison_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('complaints.csv', encoding='utf-16', sep='\\t')\n",
        "complaints = list(df['–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "\n",
        "results = process_complaints_parallel(\n",
        "    complaints=complaints[:1],\n",
        "    batch_size=100,\n",
        "    n_jobs=-1,\n",
        "    output_dir=\"parallel_results\",\n",
        "    text_column='text'\n",
        ")\n",
        "\n",
        "comparison_df = create_comparison_dataframe(\n",
        "    merged_results_file='parallel_results/all_results_merged.json',\n",
        "    original_df=df[[\"–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏\", \"–¢–∏–ø\", \"–ü–æ–¥—Ç–∏–ø\", \"–ü—Ä–µ—Ç–µ–Ω–∑–∏—è –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É\"]]\n",
        ")\n",
        "\n",
        "# comparison_df.to_csv('final_classified_complaints.csv', index=False, encoding='utf-8-sig')\n",
        "# print(\"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ final_classified_complaints.csv\")\n",
        "\n",
        "print(\"\\n=== –ü–ï–†–í–´–ï 5 –ó–ê–ü–ò–°–ï–ô ===\")\n",
        "print(comparison_df[['–º–æ–¥–µ–ª—å_–ø—Ä–æ–¥—É–∫—Ç', '–º–æ–¥–µ–ª—å_—Ç–∏–ø_–∂–∞–ª–æ–±—ã', '–º–æ–¥–µ–ª—å_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å']].head())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
