{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import re\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
        "import umap\n",
        "import plotly.express as px\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_gpt_oss_embedding(prompt, model=\"bge-m3:latest\"):\n",
        "    url = ''\n",
        "    api_key = ''\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "    response = requests.post(url, json=payload, stream=False, headers=headers, timeout=120)\n",
        "    response.raise_for_status()\n",
        "    embedding_payload = json.loads(response.content.decode('utf-8'))\n",
        "    return embedding_payload.get('embedding')\n",
        "\n",
        "\n",
        "def clean_complaint_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings(complaints, output_dir=\"embedding_results\", batch_size=100):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    results = []\n",
        "    errors = []\n",
        "    total = len(complaints)\n",
        "\n",
        "    for idx, complaint in enumerate(complaints, 1):\n",
        "        try:\n",
        "            cleaned_text = clean_complaint_text(complaint)\n",
        "            embedding = call_gpt_oss_embedding(cleaned_text)\n",
        "            embedding_dimension = len(embedding) if embedding else 0\n",
        "\n",
        "            results.append({\n",
        "                \"id\": idx,\n",
        "                \"original_text\": complaint,\n",
        "                \"cleaned_text\": cleaned_text,\n",
        "                \"embedding\": embedding,\n",
        "                \"embedding_dimension\": embedding_dimension,\n",
        "                \"processed_at\": datetime.now().isoformat()\n",
        "            })\n",
        "        except Exception as exc:\n",
        "            errors.append({\n",
        "                \"id\": idx,\n",
        "                \"original_text\": complaint,\n",
        "                \"error\": str(exc),\n",
        "                \"processed_at\": datetime.now().isoformat()\n",
        "            })\n",
        "            logging.error(f\"Ошибка при обработке записи {idx}: {exc}\")\n",
        "\n",
        "        if idx % batch_size == 0 or idx == total:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            chunk_path = os.path.join(output_dir, f\"embeddings_{timestamp}_upto_{idx}.json\")\n",
        "            payload = {\n",
        "                \"metadata\": {\n",
        "                    \"total\": total,\n",
        "                    \"saved_upto\": idx,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"processed_at\": datetime.now().isoformat()\n",
        "                },\n",
        "                \"results\": results,\n",
        "                \"errors\": errors\n",
        "            }\n",
        "            with open(chunk_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "            results.clear()\n",
        "            errors.clear()\n",
        "            logging.info(f\"Сохранен промежуточный файл: {chunk_path}\")\n",
        "\n",
        "\n",
        "def load_embedding_results(output_dir=\"embedding_results\"):\n",
        "    all_vectors = []\n",
        "    all_meta = []\n",
        "    if not os.path.exists(output_dir):\n",
        "        logging.warning(f\"Директория {output_dir} не найдена\")\n",
        "        return all_vectors, all_meta\n",
        "\n",
        "    for filename in sorted(os.listdir(output_dir)):\n",
        "        if not filename.endswith('.json'):\n",
        "            continue\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            payload = json.load(f)\n",
        "\n",
        "        for row in payload.get('results', []):\n",
        "            embedding = row.get('embedding')\n",
        "            if embedding:\n",
        "                all_vectors.append(embedding)\n",
        "                all_meta.append({\n",
        "                    \"id\": row.get('id'),\n",
        "                    \"original_text\": row.get('original_text'),\n",
        "                    \"cleaned_text\": row.get('cleaned_text'),\n",
        "                    \"embedding_dimension\": row.get('embedding_dimension'),\n",
        "                    \"processed_at\": row.get('processed_at'),\n",
        "                    \"source_file\": filename\n",
        "                })\n",
        "\n",
        "    return all_vectors, all_meta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== АНАЛИТИКА ЭМБЕДДИНГОВ ==========\n",
        "\n",
        "def prepare_embedding_matrix(output_dir=\"embedding_results\"):\n",
        "    \"\"\"Загружает все эмбеддинги, возвращает исходную и нормализованную матрицы\"\"\"\n",
        "    vectors, meta = load_embedding_results(output_dir)\n",
        "    if not vectors:\n",
        "        raise ValueError(f\"Не найдены эмбеддинги в директории {output_dir}\")\n",
        "\n",
        "    X = np.array(vectors, dtype=float)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    meta_df = pd.DataFrame(meta)\n",
        "    return X, X_scaled, meta_df, scaler\n",
        "\n",
        "\n",
        "def project_embeddings_umap(X, n_neighbors=30, min_dist=0.1, n_components=2, metric='cosine', random_state=42):\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        n_components=n_components,\n",
        "        metric=metric,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    coords = reducer.fit_transform(X)\n",
        "    return coords, reducer\n",
        "\n",
        "\n",
        "def evaluate_k_values(X, k_values, use_minibatch=True, random_state=42):\n",
        "    \"\"\"Подбор количества кластеров по силуэту\"\"\"\n",
        "    scores = []\n",
        "    for k in k_values:\n",
        "        if k < 2:\n",
        "            continue\n",
        "        if use_minibatch:\n",
        "            model = MiniBatchKMeans(n_clusters=k, random_state=random_state, batch_size=1024)\n",
        "        else:\n",
        "            model = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
        "        labels = model.fit_predict(X)\n",
        "        if len(set(labels)) < 2:\n",
        "            continue\n",
        "        score = silhouette_score(X, labels)\n",
        "        scores.append({\"k\": k, \"silhouette\": score})\n",
        "    return pd.DataFrame(scores)\n",
        "\n",
        "\n",
        "def evaluate_multiple_cluster_metrics(X, k_values, use_minibatch=True, random_state=42):\n",
        "    \"\"\"Возвращает таблицу с silhouette, Calinski-Harabasz, Davies-Bouldin и inertia\"\"\"\n",
        "    rows = []\n",
        "    for k in k_values:\n",
        "        if k < 2:\n",
        "            continue\n",
        "        if use_minibatch:\n",
        "            model = MiniBatchKMeans(n_clusters=k, random_state=random_state, batch_size=1024)\n",
        "        else:\n",
        "            model = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
        "        labels = model.fit_predict(X)\n",
        "        if len(set(labels)) < 2:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"k\": k,\n",
        "            \"silhouette\": silhouette_score(X, labels),\n",
        "            \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n",
        "            \"davies_bouldin\": davies_bouldin_score(X, labels),\n",
        "            \"inertia\": getattr(model, 'inertia_', None)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def cluster_embeddings(X, n_clusters, use_minibatch=True, random_state=42):\n",
        "    if use_minibatch:\n",
        "        model = MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state, batch_size=1024)\n",
        "    else:\n",
        "        model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')\n",
        "    labels = model.fit_predict(X)\n",
        "    return labels, model\n",
        "\n",
        "\n",
        "def plot_inertia_curve(metrics_df, title=\"Elbow curve (Inertia vs k)\"):\n",
        "    if metrics_df.empty:\n",
        "        raise ValueError(\"metrics_df пуст — нечего визуализировать\")\n",
        "    fig = px.line(\n",
        "        metrics_df.sort_values('k'),\n",
        "        x='k',\n",
        "        y='inertia',\n",
        "        markers=True,\n",
        "        title=title\n",
        "    )\n",
        "    fig.update_layout(xaxis_title='k', yaxis_title='Inertia')\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def wrap_text_for_hover(text, width=120, max_lines=None):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    lines = textwrap.wrap(text, width=width)\n",
        "    if max_lines is not None and len(lines) > max_lines:\n",
        "        lines = lines[:max_lines] + ['...']\n",
        "    return '<br>'.join(lines)\n",
        "\n",
        "\n",
        "def visualize_clusters(coords, labels, meta_df=None, title=\"Карта жалоб\", save_path=None, show=True, point_metrics=None):\n",
        "    df_plot = pd.DataFrame(coords, columns=[f\"dim_{i+1}\" for i in range(coords.shape[1])])\n",
        "    df_plot['cluster'] = labels.astype(str)\n",
        "    if meta_df is not None:\n",
        "        df_plot = pd.concat([df_plot, meta_df.reset_index(drop=True)], axis=1)\n",
        "    if point_metrics is not None:\n",
        "        df_plot = pd.concat([df_plot, point_metrics.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    text_source = df_plot.get('original_text', df_plot.get('cleaned_text', ''))\n",
        "    df_plot['hover_text'] = text_source.apply(wrap_text_for_hover)\n",
        "\n",
        "    custom_data = np.stack([\n",
        "        df_plot['cluster'],\n",
        "        df_plot.get('silhouette_point', pd.Series([np.nan] * len(df_plot))),\n",
        "        df_plot['hover_text']\n",
        "    ], axis=-1)\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_plot,\n",
        "        x='dim_1',\n",
        "        y='dim_2',\n",
        "        color='cluster',\n",
        "        title=title,\n",
        "        height=650,\n",
        "        custom_data=custom_data\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        hovertemplate=(\n",
        "            \"Кластер: %{customdata[0]}<br>\"\n",
        "            \"Silhouette: %{customdata[1]:.3f}<br>\"\n",
        "            \"%{customdata[2]}<extra></extra>\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        legend_title_text='Кластер',\n",
        "        hoverlabel=dict(\n",
        "            bgcolor='white',\n",
        "            font_color='black',\n",
        "            bordercolor='rgba(0,0,0,0.4)',\n",
        "            align='left'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if save_path:\n",
        "        fig.write_html(save_path, include_plotlyjs='inline')\n",
        "        logging.info(f\"Интерактивный график сохранён: {save_path}\")\n",
        "\n",
        "    if show:\n",
        "        fig.show()\n",
        "\n",
        "    return df_plot, fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('complaints.csv', encoding='utf-16', sep='\\t')\n",
        "complaints = list(df['Описание претензии'])\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_embeddings(\n",
        "    complaints=complaints[:10],  # замените на нужный диапазон\n",
        "    output_dir=\"embedding_results\",\n",
        "    batch_size=5\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Пример полного пайплайна: подбор k, кластеризация, визуализация ===\n",
        "\n",
        "OUTPUT_DIR = \"embedding_results\"\n",
        "\n",
        "# 1. Загружаем готовые эмбеддинги\n",
        "X_raw, X_scaled, meta_df, scaler = prepare_embedding_matrix(OUTPUT_DIR)\n",
        "print(f\"Загружено {X_raw.shape[0]} эмбеддингов размерности {X_raw.shape[1]}\")\n",
        "\n",
        "# 2. Проекция UMAP в 2D (удобно и для визуализации, и для кластеризации)\n",
        "coords_2d, umap_model = project_embeddings_umap(\n",
        "    X_scaled,\n",
        "    n_neighbors=40,\n",
        "    min_dist=0.05,\n",
        "    n_components=2,\n",
        "    metric='cosine'\n",
        ")\n",
        "print(\"UMAP готов:\", coords_2d.shape)\n",
        "\n",
        "# 3. Перебор количества кластеров. Можно поменять диапазон k_values\n",
        "k_values = range(5, 31, 5)\n",
        "score_df = evaluate_k_values(coords_2d, k_values)\n",
        "display(score_df.sort_values('silhouette', ascending=False))\n",
        "\n",
        "metrics_df = evaluate_multiple_cluster_metrics(coords_2d, k_values)\n",
        "display(metrics_df.sort_values('silhouette', ascending=False))\n",
        "plot_inertia_curve(metrics_df)\n",
        "\n",
        "best_k = int(score_df.loc[score_df['silhouette'].idxmax(), 'k']) if not score_df.empty else 5\n",
        "print(f\"Выбранное k: {best_k}\")\n",
        "\n",
        "# 4. Кластеризация (MiniBatchKMeans по UMAP координатам)\n",
        "cluster_labels, cluster_model = cluster_embeddings(coords_2d, n_clusters=best_k)\n",
        "print(\"Кластеризация завершена. Всего кластеров:\", len(np.unique(cluster_labels)))\n",
        "\n",
        "point_metrics_df = pd.DataFrame({\n",
        "    'silhouette_point': silhouette_samples(coords_2d, cluster_labels)\n",
        "})\n",
        "\n",
        "# 5. Визуализация и выгрузка таблицы с координатами\n",
        "cluster_map_df, cluster_fig = visualize_clusters(\n",
        "    coords_2d,\n",
        "    cluster_labels,\n",
        "    meta_df=meta_df,\n",
        "    title=f\"UMAP + KMeans (k={best_k})\",\n",
        "    save_path=os.path.join(OUTPUT_DIR, f\"cluster_map_k{best_k}.html\"),\n",
        "    show=False,\n",
        "    point_metrics=point_metrics_df\n",
        ")\n",
        "\n",
        "cluster_fig.show()\n",
        "cluster_map_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Этап 1. Подбор параметров UMAP (фиксируем k для черновой оценки) ===\n",
        "OUTPUT_DIR = \"embedding_results\"\n",
        "\n",
        "if 'X_scaled' not in globals() or 'meta_df' not in globals():\n",
        "    X_raw, X_scaled, meta_df, scaler = prepare_embedding_matrix(OUTPUT_DIR)\n",
        "    print(f\"Загружено {X_raw.shape[0]} эмбеддингов размерности {X_raw.shape[1]}\")\n",
        "\n",
        "umap_param_grid = [\n",
        "    {\"n_neighbors\": 20, \"min_dist\": 0.01},\n",
        "    {\"n_neighbors\": 40, \"min_dist\": 0.05},\n",
        "    {\"n_neighbors\": 60, \"min_dist\": 0.2}\n",
        "]\n",
        "\n",
        "umap_eval_rows = []\n",
        "fixed_k_for_umap = 50  # зафиксировали число кластеров для сравнения UMAP-параметров\n",
        "\n",
        "for params in umap_param_grid:\n",
        "    coords_tmp, _ = project_embeddings_umap(\n",
        "        X_scaled,\n",
        "        n_neighbors=params[\"n_neighbors\"],\n",
        "        min_dist=params[\"min_dist\"],\n",
        "        n_components=2,\n",
        "        metric='cosine'\n",
        "    )\n",
        "    labels_tmp, _ = cluster_embeddings(coords_tmp, n_clusters=fixed_k_for_umap)\n",
        "    sil = silhouette_score(coords_tmp, labels_tmp)\n",
        "    umap_eval_rows.append({\n",
        "        \"n_neighbors\": params[\"n_neighbors\"],\n",
        "        \"min_dist\": params[\"min_dist\"],\n",
        "        \"silhouette\": sil\n",
        "    })\n",
        "\n",
        "umap_eval_df = pd.DataFrame(umap_eval_rows)\n",
        "display(umap_eval_df.sort_values('silhouette', ascending=False))\n",
        "\n",
        "# выберем лучший набор UMAP-параметров\n",
        "if not umap_eval_df.empty:\n",
        "    best_umap_params = umap_eval_df.sort_values('silhouette', ascending=False).iloc[0]\n",
        "    print(\"Лучшие параметры UMAP:\", best_umap_params.to_dict())\n",
        "else:\n",
        "    best_umap_params = {\"n_neighbors\": 40, \"min_dist\": 0.05}\n",
        "\n",
        "# пересчитываем UMAP с лучшими параметрами\n",
        "coords_2d, umap_model = project_embeddings_umap(\n",
        "    X_scaled,\n",
        "    n_neighbors=int(best_umap_params[\"n_neighbors\"]),\n",
        "    min_dist=float(best_umap_params[\"min_dist\"]),\n",
        "    n_components=2,\n",
        "    metric='cosine'\n",
        ")\n",
        "print(\"Пересчитан UMAP с лучшими параметрами:\", coords_2d.shape)\n",
        "\n",
        "# === Этап 2. Подбор количества кластеров на выбранной проекции ===\n",
        "k_values = range(20, 121, 5)\n",
        "metrics_df = evaluate_multiple_cluster_metrics(coords_2d, k_values)\n",
        "display(metrics_df.sort_values('silhouette', ascending=False))\n",
        "plot_inertia_curve(metrics_df)\n",
        "\n",
        "if not metrics_df.empty:\n",
        "    best_k = int(metrics_df.sort_values('silhouette', ascending=False).iloc[0]['k'])\n",
        "else:\n",
        "    best_k = 50\n",
        "print(f\"Оптимальное k по silhouette: {best_k}\")\n",
        "\n",
        "cluster_labels, cluster_model = cluster_embeddings(coords_2d, n_clusters=best_k)\n",
        "print(\"Кластеризация завершена. Всего кластеров:\", len(np.unique(cluster_labels)))\n",
        "\n",
        "point_metrics_df = pd.DataFrame({\n",
        "    'silhouette_point': silhouette_samples(coords_2d, cluster_labels)\n",
        "})\n",
        "\n",
        "cluster_map_df, cluster_fig = visualize_clusters(\n",
        "    coords_2d,\n",
        "    cluster_labels,\n",
        "    meta_df=meta_df,\n",
        "    title=f\"UMAP + KMeans (k={best_k})\",\n",
        "    save_path=os.path.join(OUTPUT_DIR, f\"cluster_map_k{best_k}.html\"),\n",
        "    show=False,\n",
        "    point_metrics=point_metrics_df\n",
        ")\n",
        "cluster_fig.show()\n",
        "cluster_map_df.head()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
